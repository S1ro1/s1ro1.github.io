[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My name is Matej, but everyone calls me Siro. Iâ€™m a student of Artificial Intelligence and Machine Learning @ Brno University of Technology.\nI work at ðŸ¤— Hugging Face, where you can mostly find me working on distributed training and inference in accelerate and transformers. Iâ€™m also one of the core maintainers of Cluster Bot.\nAs you might have guessed, Iâ€™m mostly interested in AI and where it intersects with systems programming. I liked AI before, but I was getting more and more annoyed by waiting for my training runs to finish, so I switched to making the runs go brrr ðŸš€.\n\n\nWhen I feel like it, I like to write about random topics from the world of AI and HPC. If any of this interests you, I believe you can find something useful below. Anything from chip design to distributed trainingâ€¦"
  },
  {
    "objectID": "index.html#articles",
    "href": "index.html#articles",
    "title": "About",
    "section": "",
    "text": "When I feel like it, I like to write about random topics from the world of AI and HPC. If any of this interests you, I believe you can find something useful below. Anything from chip design to distributed trainingâ€¦"
  },
  {
    "objectID": "articles/deepcompile.html",
    "href": "articles/deepcompile.html",
    "title": "DeepCompile",
    "section": "",
    "text": "In release 0.16.6 of DeepSpeed, you can find a new interesting feature. Itâ€™s called DeepCompile, in the release notes itâ€™s hidden as the last feature (who reads those, right?) and if friend didnâ€™t tag me with DeepSpeedâ€™s post on X, I would have probably missed it. So why is it interesting?\nDeepSpeed is a framework for distributed training, itâ€™s a pretty big library, most notable feature being probably the implementation of ZeRO. With DeepCompile, they enable compiler-based optimizations for ZeRO algorithms. Why should we care you might ask? There currently is a lot of work going on in the field of optimizing distributed training. The common problem is that communication operations (all-gather, reduce-scatter, etc.) which are ever-present in the training loop, are inserted at runtime, making them hard to optimize.\n\n\n\n\n\n\nNote\n\n\n\nIn the rest of this article, Iâ€™ll assume you are familiar with communication primitives and basics of distributed training."
  },
  {
    "objectID": "articles/deepcompile.html#zero-training-loop",
    "href": "articles/deepcompile.html#zero-training-loop",
    "title": "DeepCompile",
    "section": "ZeRO training loop",
    "text": "ZeRO training loop\nI mentioned that each GPU holds only a partition of the model, optimizer state and gradients. But how can the training loop work then? Before a part of the model is used (its .forward() method is called), it is all-gathered across all GPUs. Then after its forward pass, the memory is freed (usually, though this can be controlled by the user). When this layer is again needed for backward pass, itâ€™s all-gathered again (if it was freed before). Then at the optimizer step, each GPU is responsible for updating only its partition of the model.\n Figure 1: Visualization of the ZeRO training loop showing how model parameters are partitioned across GPUs, all-gathered when needed for forward/backward passes, and then freed to conserve memory."
  },
  {
    "objectID": "articles/deepcompile.html#prefetching",
    "href": "articles/deepcompile.html#prefetching",
    "title": "DeepCompile",
    "section": "Prefetching",
    "text": "Prefetching\nIf youâ€™re already familiar with this, or curious enough, you might have identified part of the problem. GPUs are very good at overlapping communication and computation. This allows us to start the all-gather for the current layer while the previous layer is still present in the GPU memory. Why canâ€™t we start even earlier, beginning the all-gather operation before the pre-previous layer? We can! This method is called prefetching. But how do we know when to stop/start the prefetching? If we start too early, we risk having too many layers in the memory -&gt; RuntimeError: CUDA error: out of memory. If we start too late, the layer might not be gathered in time for its forward pass -&gt; slow training ðŸ’¤. PyTorch implementation addresses this by prefetching maximum of 1 layer ahead, therefore maximum of 2 layers are in memory at the same time. But this is only a somewhat conservative heuristic. We can do better (and DeepCompile does exactly that, more on that later)."
  },
  {
    "objectID": "articles/deepcompile.html#resharding",
    "href": "articles/deepcompile.html#resharding",
    "title": "DeepCompile",
    "section": "Resharding",
    "text": "Resharding\nIf we have enough memory left, do we need to free the memory after the forward pass of the layer? No, we donâ€™t! PyTorch controls this via options like ShardingStrategy.SHARD_GRAD_OP for FSDP1 and reshard_after_forward=False for FSDP2. This way, the layer is kept in memory after its foward pass until itâ€™s again needed for backward. But this is again a binary choice, either we keep the layer in memory or we donâ€™t. We can do better. Again, DeepCompile does!"
  },
  {
    "objectID": "articles/deepcompile.html#offloading",
    "href": "articles/deepcompile.html#offloading",
    "title": "DeepCompile",
    "section": "Offloading",
    "text": "Offloading\nSome memory parts, like the optimizer state, are not needed for the training loop. The optimizer state is only needed for the parameter update (after the backward pass). Do we need to keep it in GPU memory? No, we donâ€™t. But how do we know how much we can offload? Offloading too much takes too long and offloading too little results in few memory savings. There again isnâ€™t a good general solution. DeepCompile has one."
  },
  {
    "objectID": "articles/deepcompile.html#optimization-passes",
    "href": "articles/deepcompile.html#optimization-passes",
    "title": "DeepCompile",
    "section": "Optimization passes",
    "text": "Optimization passes\n\nProactive Prefetching In the Prefetching section, we discussed how all-gather operations can be initialized before the layer is used. This pass does exactly that. Based on the memory usage from the profiler, it attempts to schedule the all-gathers operations as early as possible, while respecting the memory limit. So how would this look in the diagram?\n\n\n\n\nProactive Prefetching\n\n\nFigure 3: Proactive prefetching: The green line represents the new memory usage, it is higher than the original one, but stays below the memory limit. You can see that we prefetch as early as possible, maximizing the memory, but staying below the limit. I think itâ€™s easier to understand this in code, so hereâ€™s how it could be implemented:\ndef proactive_prefetching_pass(graph):\n    unscheduled_all_gathers = []\n    new_graph = Graph()\n\n    for node in reversed(graph):\n        if not isinstance(node, AllGather): \n            # if it's not an all-gather, add to the new graph\n            new_graph.append(node)\n        else:\n            # Get memory requirements for all unscheduled all gathers\n            unscheduled_mem_usage = sum([\n                node.memory_usage for node in unscheduled_all_gathers\n            ]) + node.memory_usage # current all gather\n\n            # check if current all gather still fits into the memory limit\n            # profiler.current_memory_usage returns the memory usage at the current step\n            total_required_mem = (  \n                unscheduled_mem_usage + profiler.current_memory_usage(node)\n            ) \n            if total_required_mem &lt; memory_limit:\n                # if it fits, add to the unscheduled all gathers\n                unscheduled_all_gathers.append(node)\n            else:\n                # if it doesn't fit, schedule the current all gather\n                scheduled_all_gathers = fuse(unscheduled_all_gathers)\n                new_graph.append(scheduled_all_gathers)\n                unscheduled_all_gathers = [node]\n\n    # schedule the last unscheduled all gathers\n    last_scheduled_all_gathers = fuse(unscheduled_all_gathers)\n    new_graph.append(last_scheduled_all_gathers)\n    return new_graph\nAlgorithm 1: Proactive Prefetching\nThis pass traverses the graph in reverse order, collects all-gather operations that fit within the memory limit, and as adding another would exceed the memory limit, fuses them together and schedules them, effectively moving them as early as possible. You can notice a fuse operation we havenâ€™t talked about. This function fuses multiple all-gather operations into a so-called bucket. This is done because communication operations involving small data sizes can be inefficient.\n\nSelective Resharding In the Resharding section, we discussed how we can keep the layer in memory after using it in forward, then reusing it in backward. This pass does exactly that. If there is still some memory left after applying the previous pass, we can use this available memory to keep some layers unsharded (i.e.Â not freeing the memory after the forward pass). We decide on which layers to keep, using the following heuristic:\n\n\\[\n\\frac{\\text{communication\\_time}_{op_i}}{\\text{data\\_size}_{op_i}}\n\\] , where \\(op_i\\) is the i-th operation in the graph and \\(\\text{communication\\_time}_{op_i}\\) is the time taken for its communication and \\(\\text{data\\_size}_{op_i}\\) is the size of the data communicated. The goal is to esentially maximize the communication time saved, while minimizing the memory usage.\n\nAdaptive Offloading\n\nAgain, this pass is very simple, DeepCompile basically detects if weâ€™re lacking memory, and if so, it partially offloads the optimizer state to the CPU. When sufficient memory becomes available again, it begins reloading the state back to the GPU. This way, we donâ€™t go OOM, however, we avoid unecessary time spent transferring data to the CPU and back."
  },
  {
    "objectID": "articles/transformers-tp.html",
    "href": "articles/transformers-tp.html",
    "title": "Tensor Parallelism in Transformers",
    "section": "",
    "text": "With the advent of large language (and now multi-modal) models, there is a growing need for efficient ways to train and serve these models. Models with 100s of billions of parameters are not uncommon and those do not fit into a single GPU. Tensor parallelism is one of the techniques that can be used to help with this problem. This article will cover the basics of tensor parallelism, how it works and most importantly, how you can use predefined tensor parallelism APIs in ðŸ¤— Transformers."
  },
  {
    "objectID": "articles/transformers-tp.html#introduction",
    "href": "articles/transformers-tp.html#introduction",
    "title": "Tensor Parallelism in Transformers",
    "section": "",
    "text": "With the advent of large language (and now multi-modal) models, there is a growing need for efficient ways to train and serve these models. Models with 100s of billions of parameters are not uncommon and those do not fit into a single GPU. Tensor parallelism is one of the techniques that can be used to help with this problem. This article will cover the basics of tensor parallelism, how it works and most importantly, how you can use predefined tensor parallelism APIs in ðŸ¤— Transformers."
  },
  {
    "objectID": "articles/transformers-tp.html#a.-how-does-tensor-parallelism-work",
    "href": "articles/transformers-tp.html#a.-how-does-tensor-parallelism-work",
    "title": "Tensor Parallelism in Transformers",
    "section": "2a. How does tensor parallelism work?",
    "text": "2a. How does tensor parallelism work?\nAt its core, tensor parallelism involves partitioning the modelâ€™s weights across multiple devices. Letâ€™s consider a simple matrix multiplication, \\(Y = XW\\), which is a fundamental operation in neural networks.\n\nColumn-wise Partitioning\n\nWe split the weight matrix \\(W\\) column-wise across two GPUs, where \\(W_1\\) resides on GPU 1 and \\(W_2\\) on GPU 2.\n\n\\[\nW = \\begin{bmatrix} W_1 & W_2 \\end{bmatrix}\n\\]\n\nThe input matrix \\(X\\) is broadcast to both GPUs (each GPU gets a full copy of \\(X\\)).\n\n\\[\nY = XW = X \\begin{bmatrix} W_1 & W_2 \\end{bmatrix} = \\begin{bmatrix} XW_1 & XW_2 \\end{bmatrix} = \\begin{bmatrix} Y_1 & Y_2 \\end{bmatrix}\n\\]\n\nEach GPU computes a part of the output matrix \\(Y\\).\n\n\\[\nY_1 = XW_1 \\quad \\text{and} \\quad Y_2 = XW_2\n\\]\n\nThe final result \\(Y\\) is obtained by concatenating \\(Y_1\\) and \\(Y_2\\) along the column dimension.\n\n\\[\nY = \\begin{bmatrix} Y_1 & Y_2 \\end{bmatrix}\n\\]\n#TODO Visualization\n\n\nRow-wise Partitioning\n\nWe split the input matrix \\(X\\) column-wise across two GPUs, where \\(X_1\\) resides on GPU 1 and \\(X_2\\) on GPU 2.\n\n\\[\nX = \\begin{bmatrix} X_1 & X_2 \\end{bmatrix}\n\\]\n\nThe weight matrix \\(W\\) is split row-wise, where \\(W_1\\) resides on GPU 1 and \\(W_2\\) on GPU 2.\n\n\\[\nW = \\begin{bmatrix} W_1 \\\\ W_2 \\end{bmatrix}\n\\]\n\nEach GPU computes a part of the output matrix \\(Y\\).\n\n\\[\nY_1 = X_1W_1 \\quad \\text{and} \\quad Y_2 = X_2W_2\n\\]\n\nThe final result \\(Y\\) is obtained by element-wise addition of \\(Y_1\\) and \\(Y_2\\).\n\n\\[\nY = Y_1 + Y_2\n\\]\n#TODO Visualization\nThis approach can get scaled to any number of GPUs by simply increasing the number of partitions. Usual approach is to apply column-wise partitioning first, then row-wise partitioning. This way, we can reduce the communication overhead between GPUs. You can see this in the image below: We do not to do anything extra after the column-wise partitioning, it conveniently splits the input across GPUs, just as row-wise partitioning needs it."
  },
  {
    "objectID": "articles/transformers-tp.html#tensor-parallelism-for-transformer-models",
    "href": "articles/transformers-tp.html#tensor-parallelism-for-transformer-models",
    "title": "Tensor Parallelism in Transformers",
    "section": "Tensor Parallelism for transformer models",
    "text": "Tensor Parallelism for transformer models\nAverage transformer architecture consists of multiple layers, each containing a self-attention and a feed-forward network. We would like to apply tensor parallelism to each of these. As mentioned earlier, we can apply column-wise partitioning first, then row-wise partitioning.\n\nFeed-Forward Network\nApplying tensor parallelism to feed-forward network is straightforward. We can apply column-wise partitioning first, then row-wise partitioning. ReLU or other activation functions are applied element-wise - we donâ€™t need any extra communication here.\n\\[\nY = \\text{FFN}(X) = \\text{ReLU}(XW_1)W_2\n\\]\n#TODO Visualization\n\n\nSelf-Attention\nSelf-attention might seem a bit tricky at first, though it is actually quite simple and natural. We split attention column-wise, making sure each GPU gets a single attention head. Then we split the output projection row-wise.\n\\[\nY = \\text{Dropout}(\\text{Self-Attention}(X)W_O)\n\\]\n#TODO Visualization"
  },
  {
    "objectID": "articles/transformers-tp.html#b.-sequence-parallelism",
    "href": "articles/transformers-tp.html#b.-sequence-parallelism",
    "title": "Tensor Parallelism in Transformers",
    "section": "2b. Sequence Parallelism",
    "text": "2b. Sequence Parallelism\nTODO! Should we discuss sequence parallelism? Used in some models (LLAMA4) - might be interesting to mention."
  },
  {
    "objectID": "articles/transformers-tp.html#using-transformers",
    "href": "articles/transformers-tp.html#using-transformers",
    "title": "Tensor Parallelism in Transformers",
    "section": "3. Using ðŸ¤— Transformers",
    "text": "3. Using ðŸ¤— Transformers\nAll of the methods mentioned above seem to be a lot of work to implement manually. Thankfully, Torch and ðŸ¤— Transformers have got you covered! With PyTorchâ€™s torch.distributed and DTensor itâ€™s very simple to implement tensor parallelism, though it still requires manual configuration for each model. This is where ðŸ¤— Transformers comes in. It provides with tensor parallelism pre-configured for some popular models, also providing a simple way to implement it for any other model.\n\nðŸ¤— Transformers Tensor Parallelism\nLetâ€™s take a look at how we can use ðŸ¤— Transformersâ€™ tensor parallelism.\nfrom transformers import AutoModelForCausalLM\n\n# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" # uncomment for smaller number of GPUs\nmodel_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" # better to visualize\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=\"auto)\n\nprint(model._tp_plan)\nThis will load the model with tensor parallelism pre-configured for the best performance. The model has an attribute _tp_plan which contains information about the tensor parallelism plan, running the above with\ntorchrun --nproc_per_node=8 main.py\nwill give a result such as:\n{\n    \"layer.*.self_attn.q_proj\": \"colwise\",\n    \"layer.*.self_attn.k_proj\": \"colwise\",\n    \"layer.*.self_attn.v_proj\": \"colwise\",\n    \"layer.*.self_attn.o_proj\": \"rowwise\",\n    ...\n}\nThis tells us that the query projection is partitioned column-wise, the key and value projections are partitioned column-wise and the output projection is partitioned row-wise. The model is directly loaded and configured with this plan.\n\n\nTODO\n\nSupported layer types (src/transformers/integrations/tensor_parallel.py) - mostly cover replicated, why local and packed\nSupport own models\nShould we first implement it in a stable API or just showcase the current way to do it (setting model.config.tp_plan and supports_tp)?\nSupported models\nTraining/Inference (Accelerate for training)"
  },
  {
    "objectID": "articles/quantization-aware-training.html",
    "href": "articles/quantization-aware-training.html",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "In this tutorial, we will be looking at the concept of quantization aware training, how does it work in depth, its benefits and how to implement it in PyTorch. To properly understand this concept, proper understanding of quantization basics is required.\n\n\nFormally, quantization is the process of constraining an input from a continuous or otherwise large set of values to a discrete set of values. You can think of it as a way to reduce the precision of the data. In neural networks, quantization is the process of reducing the precision of the weights and activations. This can be helpful in different ways.\n\nMemory Reduction: In the example of current LLMs, the weights of the feed-forward layers are quite large. Imagine a forward layer weight matrix in Llama 3 70B Model, the weight matrix could be of size 8192 * 8192. In case of float16, this weight matrix would require 8192 * 8192 * 2 = 134,217,728 bytes of memory (approximately 128 MB). This is a lot of memory to store and process, when we consider the fact that the model has multiple such layers. In case we reduced the precision of the weights, we can reduce the load times from memory approximately two, four-fold respectively when using int8 or int4 data types.\nSpeedup: Quantization can also help in speeding up the inference, sometimes even the training process. From computer architecture perspective, the operations on large data types, such as float16 or float32 are expensive and slow. These operations are way faster and cheaper when performed on smaller data types like int8 or float8. When we take a look at the current state-of-the-art GPU Nvidia H100 and its datasheet, we can see that the performance of the GPU Tensor Cores linearly increases with the decrease in the data type size.\n\nNow, that we have a basic understanding on why quantization is important, letâ€™s take a look at how quantization works.\nImportant To simplify things, we will only be looking at quantization to a lower precision data type, that exists in the PyTorch framework, to avoid hassles of binary operations. That is, from torch.float16 to torch.int8. Also, we will be considering a method called Linear Quantization. This is the most common method of quantization.\n\n\n\nIf we think of int8 as a data type, it can store values in the range of [-128, 127]. However, our weights and activations in float16 have a range of [-65504, 65504]. Also, this range in float16 is not uniformly distributed, therefore accommodating a lot more possible values. To quantize the weights and activations, we need to map the values in float16 to the range of int8. This can be done by the following steps:\n\nMin and Max Calculation: We need to calculate the minimum and maximum values in the data. This will tell us what values map to int8.min and int8.max.\n\n Figure 1: Visual representation of min and max calculation, W_max and W_min are the maximum and minimum values in tensor to be quantized, these values then map to int8.max and int8.min respectively.\n\nZero Point Calculation: We can think of zero point as the point where the float16 value of 0 lies in the int8 data type. This basically maps the real number r=0 to a quantized integer.\n\n Figure 2: Visual representation of zero point calculation. Z on the quantized axis is the zero point, and represents where the r=0.0 lies on the quantized axis.\n\nScale Calculation: The scale basically tells us, how much each unit in the quantized data type represents in the original data type. Imagine a scale of 1.0, this means that each unit in the quantized data type represents 1.0 in the original data type. The larger the scale, the larger is the original input range.\n\nAfter these steps, we have everything we need to quantize and dequantize the data. With r being the real number, q being the quantized number, Z being the zero point, and S being the scale, the quantization and dequantization can be done by the following equations:\n\\[\nq = \\text{round}\\left(\\frac{r}{S}\\right) + Z\n\\]\n\\[\nr = (q - Z) \\cdot S\n\\]\nWith some additional math, we can also derive the scale and zero point equations from the min and max values.\n\\[\nS = \\frac{W_{max} - W_{min}}{Q_{max} - Q_{min}} = \\frac{W_{max} - W_{min}}{127 - (-128)}\n\\]\n\\[\nZ = \\text{round}\\left(Q_{min} - \\frac{W_{min}}{S}\\right) = \\text{round}\\left(-128 - \\frac{W_{min}}{S}\\right)\n\\]\n\n\n\nimport torch\nfrom collections import namedtuple\n\nQTensor = namedtuple(\n    \"QTensor\", [\"tensor\", \"scale\", \"zero_point\"]\n)  # we need to track the scale and zero point to dequantize the tensor later\n\n\ndef quantize_tensor(tensor: torch.Tensor) -&gt; QTensor:\n    W_min = tensor.min()\n    W_max = tensor.max()\n\n    Q_min = torch.iinfo(torch.int8).min  # Get the minimum value of the int8 data type\n    Q_max = torch.iinfo(torch.int8).max  # Get the maximum value of the int8 data type\n\n    S = (W_max - W_min) / (Q_max - Q_min)  # Calculate the scale\n    Z = torch.round(Q_min - (W_min / S))  # Calculate the zero point\n\n    quantized_tensor = torch.round(tensor / S) + Z  # Quantize the tensor\n\n    return QTensor(\n        tensor=quantized_tensor.to(torch.int8), scale=S, zero_point=Z\n    )  # Return the quantized tensor, scale, and zero point\n\n\ndef dequantize_tensor(q_tensor: QTensor) -&gt; torch.Tensor:\n    return (\n        q_tensor.tensor.to(torch.float16) - q_tensor.zero_point\n    ) * q_tensor.scale  # simply compute the real value from the already computed data\n\n\nYou might have noticed that we lose quite a lot of information while quantizing the tensor. This might lead to a precision loss, which can be detrimental to the performance of the model. Imagine a scenario where our input tensor distribution looks like the following:\n Figure 3: A pretty common distribution of weights, where most of the values are centered around 0.0.\nNow imagine, we have a single data-point, which is far from the distribution, letâ€™s say W_max=1000.0. If we try to quantize this tensor, the scale would be very large, therefore a distance of 1 in the quantized data would represent a very large distance in the original unquantized data. But remember, our input tensor is distributed around 0.0, with most values lying in the range of [-10.0, 10.0]. This means that most of these values would be quantized to the same value, therefore losing a lot of information leading to a loss in performance.\nTo fix this issue, we can use a method called Clipping. This method involves clipping the values of the tensor to a certain range, and then quantizing the tensor. Our PyTorch implementation can be extended to include this method by the following:\ndef quantize_tensor(\n    tensor: torch.Tensor, clip_min: float | None = None, clip_max: float | None = None\n) -&gt; QTensor:\n    if clip_min or clip_max: # check if atleast one of the clip values is provided\n        tensor = torch.clamp(tensor, clip_min, clip_max)\n\n    W_min = tensor.min()\n    W_max = tensor.max()\n\n    Q_min = torch.iinfo(torch.int8).min  # Get the minimum value of the int8 data type\n    Q_max = torch.iinfo(torch.int8).max  # Get the maximum value of the int8 data type\n\n    S = (W_max - W_min) / (Q_max - Q_min)  # Calculate the scale\n    Z = torch.round(Q_min - (W_min / S))  # Calculate the zero point\n\n    quantized_tensor = torch.round(tensor / S) + Z  # Quantize the tensor\n\n    return QTensor(\n        tensor=quantized_tensor.to(torch.int8), scale=S, zero_point=Z\n    )  # Return the quantized tensor, scale, and zero point\nThis is the only required change to the implementation. In this implementation, we choose the clipping values manually, but in production cases, the clipping values are usually computed from the data distribution via different methods, such as Percentile Clipping, or even optimization methods such as minimizing the KL Divergence between the original and dequantized distribution. This process is called Calibration.\n\n\n\nAnother method to improve the performance of the quantized model is called Quantization Granularity. With the above implementation, we are computing the scale and zero point for the entire tensor. We could improve on this, by computing these values for a sub-part of the tensor. There are different variants of tensor splitting, such as Per Channel, Per Token, etc. The only difference between these variants is across which dimension is the zero point and scale computed. This can further lead to a better performance of the model, with cost of only a few extra bytes in memory. To save time and space, we will not be implementing these methods from scratch here, but just have this in mind when currently used quantization schemes are shown.\n\n\n\n\nWith this out of the way, we can finally take a look at the concept of Quantization Aware Training. To further improve the performance of the quantized model at inference time, we can use the concept of Quantization Aware Training. This method involves making the model used to quantized weights, activations respectively. This involves training or fine-tuning the model with something called Fake Quantization. This is a method to simulate the quantization process during training. This is accomplished by doing the following:\n\nOriginal weights are stored in the original data type, such as float16.\nComputation is done in the original data type.\nAfter we load the weights, we quantize them and then dequantize back. This is done to simulate the loading of integer weights and their dequantization done during the inference.\nThe activations of the previous layer can be quantized, then dequantized back to get the values that the model would be using during the inference. This depends whether weâ€™re doing both activation and weight quantization, or only weight quantization.\nWe then backpropagate the gradient w.r.t. the dequantized weights, and update the original weights.\n\nYou might be wondering, why do we compute with the dequantized weights, but update the original weights? If you remember our implementation, to dequantize the tensor, we round the values to the nearest integer. This means that if the gradient w.r.t. the dequantized weights is used to update the dequantized weights, the change could be too small to be further visible in the integer representation, therefore we would lose the update information. We can think of this as passing the gradient to the original weights. This is called STE or Straight Through Estimator.\nItâ€™s easier to see this in a diagram:\n Figure 4: Visual representation of Quantization Aware Training. In this diagram, we can see that the weights are stored in the original data type, and are quantized and dequantized during the forward pass. This simulates the inference process, where the weights are loaded from memory and then dequantized. During the backward pass, we pass the gradient to the original weights, which are then updated.\n\n\n\nTo properly implement this, we would like to replace all of the torch.nn.Linear layers with our own custom implementation. This custom implementation would involve the following:\n\nQuantizing the weights\nDequantizing the weights.\nComputing the forward pass with the dequantized weights.\nBackpropagating the error w.r.t. the dequantized weights.\nUpdating the original weights with the gradient.\n\nTo do this, we can register a custom autograd function in PyTorch.\nclass FakeQuantizeFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx: torch.autograd.function.FunctionCtx,\n        tensor: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        W_min, W_max = tensor.min(), tensor.max()\n        Q_min, Q_max = torch.iinfo(torch.int8).min, torch.iinfo(torch.int8).max\n        scale = (W_max - W_min) / (Q_max - Q_min)\n        zero_point = torch.round(-Q_min - (W_min / scale))\n\n        # Quantize\n        quantized = torch.round(tensor / scale) + zero_point\n        quantized = torch.clamp(quantized, Q_min, Q_max)\n\n        # Dequantize\n        dequantized = (quantized - zero_point) * scale\n\n        # Save mask for backward pass\n        mask = (quantized &gt;= Q_min) & (quantized &lt;= Q_max)\n        ctx.save_for_backward(mask)\n\n        return dequantized\n\n    @staticmethod\n    def backward(\n        ctx: torch.autograd.function.FunctionCtx, \n        grad_output: torch.Tensor\n    ) -&gt; tuple[torch.Tensor]:\n        (mask,) = ctx.saved_tensors\n        return grad_output * mask\nThis function is used to simulate the quantization process during training. Note the mask variable, this is used to store a boolean mask of values that werenâ€™t clipped during the forward pass. Therefore, values that were clipped are not updated in the backward pass. This helps simulate the inference process and stabilizes the training process.\nAfter this, we can create a torch.nn.Module that encapsulates the FakeQuantizeFunction and replace all of the torch.nn.Linear layers with this module. We can do this using a simple utility function.\nclass QuantizedLinear(torch.nn.Linear):\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        # Quantize weights\n        quantized_weight = FakeQuantizeFunction.apply(self.weight)\n\n        # Use quantized weights for the linear operation\n        return torch.nn.functional.linear(input, quantized_weight, self.bias)\ndef replace_layers_with_quantized(model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"Replaces all Linear layers with QuantizedLinear layers\"\"\"\n    for name, module in model.named_children():\n        if isinstance(module, torch.nn.Linear):\n            setattr(model, name, QuantizedLinear(\n                in_features=module.in_features,\n                out_features=module.out_features,\n                bias=module.bias is not None\n            ))\n            # Copy the weights and bias\n            getattr(model, name).weight.data = module.weight.data\n            if module.bias is not None:\n                getattr(model, name).bias.data = module.bias.data\n        else:\n            replace_layers_with_quantized(module)\n    return model\nNow, we can create our own model and use this utility function to do QAT.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(784, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 10)\n)\n\nmodel = replace_layers_with_quantized(model)\n\n# Now we can train the model as usual\n# ...\n\n\n\nAs you might have noticed, we have written quite a lot of code, which doesnâ€™t handle a lot of edge cases and is rather simple. We can use PyTorchâ€™s AO library to do this for us. This library provides a QAT module, which provides functionality for different quantization schemes.\nThe examples shown were purely educational. In production, different quantization schemes are used to improve the performance of the model. AO currently provides 2 different quantization schemes (note the different granularities of quantization, as discussed in the Quantization Granularity section), which are:\n\nint8 per token dynamic activation quantization with int4 per group weight quantization: This method quantizes weights to int8 and activations to int4. Then, computation is done in original data type, that is float16 usually. This is a good starting point for quantization aware training.\nint4 per group weight quantization: This method quantizes weights to int4, but keeps the activations in float16. Then, weights are dequantized on the fly during the matmul kernel call. This is just to optimize the latency and performance of the model.\n\nTo reproduce our example with AO, you can use the following code:\nimport torch\nfrom torchao.quantization.qat import Int8DynActInt4WeightQATQuantizer\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(784, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 10)\n).cuda()\n\n# Quantizer for int8 dynamic per token activations +\n# int4 grouped per channel weights, only for linear layers\nqat_quantizer = Int8DynActInt4WeightQATQuantizer()\n\n# Insert \"fake quantize\" operations into linear layers.\nmodel = qat_quantizer.prepare(model)\n\n# Now we can train the model as usual\n# ...\nRecalling our example, we have replaced the torch.nn.Linear layers with our own custom implementation. Equivalent of this is done by the qat_quantizer.prepare(model) method. This method inserts the FakeQuantizeFunction into the linear layers, and replaces the weights in the matrix multiplication with the quantized weights.\nHowever, in case of inference, we do not want to do these steps. We only need to dequantize the weights and do the computation, as the weights are already quantized when loaded from memory. We havenâ€™t implemented this in our example, as its not relevant to the concept, but is required in production setting. AO provides a qat_quantizer.convert(model) method, which does this for us. We can use the following code to achieve this:\n# Convert the model to the quantized model\n# This replaces all the \"fake quantize\" operations with the actual quantize operations\nmodel = qat_quantizer.convert(model)\n\n# Now we can use the model for inference\n# ...\n\n\n\n\nFinetuning: finetuning the model with QAT is usually a better approach then training the model from scratch.\nLayers: quantizing only some layers is a good approach, some layers are influenced more by the quantization process. Try experimenting with replacing only some layers. Replacing the later layers is usually better than replacing the earlier layers. Also, in general itâ€™s not a good approach to quantize critical layers, such as attention. A good approach is to quantize the feed-forward layers, as those are the ones that require the most memory.\n\n\n\n\nIn this tutorial, we have looked at the concept of quantization aware training, how does it work in depth, its benefits and how to implement it from scratch in PyTorch, and how to use AO to do this for us, which is a lot more efficient approach."
  },
  {
    "objectID": "articles/quantization-aware-training.html#what-is-quantization",
    "href": "articles/quantization-aware-training.html#what-is-quantization",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "Formally, quantization is the process of constraining an input from a continuous or otherwise large set of values to a discrete set of values. You can think of it as a way to reduce the precision of the data. In neural networks, quantization is the process of reducing the precision of the weights and activations. This can be helpful in different ways.\n\nMemory Reduction: In the example of current LLMs, the weights of the feed-forward layers are quite large. Imagine a forward layer weight matrix in Llama 3 70B Model, the weight matrix could be of size 8192 * 8192. In case of float16, this weight matrix would require 8192 * 8192 * 2 = 134,217,728 bytes of memory (approximately 128 MB). This is a lot of memory to store and process, when we consider the fact that the model has multiple such layers. In case we reduced the precision of the weights, we can reduce the load times from memory approximately two, four-fold respectively when using int8 or int4 data types.\nSpeedup: Quantization can also help in speeding up the inference, sometimes even the training process. From computer architecture perspective, the operations on large data types, such as float16 or float32 are expensive and slow. These operations are way faster and cheaper when performed on smaller data types like int8 or float8. When we take a look at the current state-of-the-art GPU Nvidia H100 and its datasheet, we can see that the performance of the GPU Tensor Cores linearly increases with the decrease in the data type size.\n\nNow, that we have a basic understanding on why quantization is important, letâ€™s take a look at how quantization works.\nImportant To simplify things, we will only be looking at quantization to a lower precision data type, that exists in the PyTorch framework, to avoid hassles of binary operations. That is, from torch.float16 to torch.int8. Also, we will be considering a method called Linear Quantization. This is the most common method of quantization."
  },
  {
    "objectID": "articles/quantization-aware-training.html#how-does-quantization-work",
    "href": "articles/quantization-aware-training.html#how-does-quantization-work",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "If we think of int8 as a data type, it can store values in the range of [-128, 127]. However, our weights and activations in float16 have a range of [-65504, 65504]. Also, this range in float16 is not uniformly distributed, therefore accommodating a lot more possible values. To quantize the weights and activations, we need to map the values in float16 to the range of int8. This can be done by the following steps:\n\nMin and Max Calculation: We need to calculate the minimum and maximum values in the data. This will tell us what values map to int8.min and int8.max.\n\n Figure 1: Visual representation of min and max calculation, W_max and W_min are the maximum and minimum values in tensor to be quantized, these values then map to int8.max and int8.min respectively.\n\nZero Point Calculation: We can think of zero point as the point where the float16 value of 0 lies in the int8 data type. This basically maps the real number r=0 to a quantized integer.\n\n Figure 2: Visual representation of zero point calculation. Z on the quantized axis is the zero point, and represents where the r=0.0 lies on the quantized axis.\n\nScale Calculation: The scale basically tells us, how much each unit in the quantized data type represents in the original data type. Imagine a scale of 1.0, this means that each unit in the quantized data type represents 1.0 in the original data type. The larger the scale, the larger is the original input range.\n\nAfter these steps, we have everything we need to quantize and dequantize the data. With r being the real number, q being the quantized number, Z being the zero point, and S being the scale, the quantization and dequantization can be done by the following equations:\n\\[\nq = \\text{round}\\left(\\frac{r}{S}\\right) + Z\n\\]\n\\[\nr = (q - Z) \\cdot S\n\\]\nWith some additional math, we can also derive the scale and zero point equations from the min and max values.\n\\[\nS = \\frac{W_{max} - W_{min}}{Q_{max} - Q_{min}} = \\frac{W_{max} - W_{min}}{127 - (-128)}\n\\]\n\\[\nZ = \\text{round}\\left(Q_{min} - \\frac{W_{min}}{S}\\right) = \\text{round}\\left(-128 - \\frac{W_{min}}{S}\\right)\n\\]"
  },
  {
    "objectID": "articles/quantization-aware-training.html#implementation",
    "href": "articles/quantization-aware-training.html#implementation",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "import torch\nfrom collections import namedtuple\n\nQTensor = namedtuple(\n    \"QTensor\", [\"tensor\", \"scale\", \"zero_point\"]\n)  # we need to track the scale and zero point to dequantize the tensor later\n\n\ndef quantize_tensor(tensor: torch.Tensor) -&gt; QTensor:\n    W_min = tensor.min()\n    W_max = tensor.max()\n\n    Q_min = torch.iinfo(torch.int8).min  # Get the minimum value of the int8 data type\n    Q_max = torch.iinfo(torch.int8).max  # Get the maximum value of the int8 data type\n\n    S = (W_max - W_min) / (Q_max - Q_min)  # Calculate the scale\n    Z = torch.round(Q_min - (W_min / S))  # Calculate the zero point\n\n    quantized_tensor = torch.round(tensor / S) + Z  # Quantize the tensor\n\n    return QTensor(\n        tensor=quantized_tensor.to(torch.int8), scale=S, zero_point=Z\n    )  # Return the quantized tensor, scale, and zero point\n\n\ndef dequantize_tensor(q_tensor: QTensor) -&gt; torch.Tensor:\n    return (\n        q_tensor.tensor.to(torch.float16) - q_tensor.zero_point\n    ) * q_tensor.scale  # simply compute the real value from the already computed data\n\n\nYou might have noticed that we lose quite a lot of information while quantizing the tensor. This might lead to a precision loss, which can be detrimental to the performance of the model. Imagine a scenario where our input tensor distribution looks like the following:\n Figure 3: A pretty common distribution of weights, where most of the values are centered around 0.0.\nNow imagine, we have a single data-point, which is far from the distribution, letâ€™s say W_max=1000.0. If we try to quantize this tensor, the scale would be very large, therefore a distance of 1 in the quantized data would represent a very large distance in the original unquantized data. But remember, our input tensor is distributed around 0.0, with most values lying in the range of [-10.0, 10.0]. This means that most of these values would be quantized to the same value, therefore losing a lot of information leading to a loss in performance.\nTo fix this issue, we can use a method called Clipping. This method involves clipping the values of the tensor to a certain range, and then quantizing the tensor. Our PyTorch implementation can be extended to include this method by the following:\ndef quantize_tensor(\n    tensor: torch.Tensor, clip_min: float | None = None, clip_max: float | None = None\n) -&gt; QTensor:\n    if clip_min or clip_max: # check if atleast one of the clip values is provided\n        tensor = torch.clamp(tensor, clip_min, clip_max)\n\n    W_min = tensor.min()\n    W_max = tensor.max()\n\n    Q_min = torch.iinfo(torch.int8).min  # Get the minimum value of the int8 data type\n    Q_max = torch.iinfo(torch.int8).max  # Get the maximum value of the int8 data type\n\n    S = (W_max - W_min) / (Q_max - Q_min)  # Calculate the scale\n    Z = torch.round(Q_min - (W_min / S))  # Calculate the zero point\n\n    quantized_tensor = torch.round(tensor / S) + Z  # Quantize the tensor\n\n    return QTensor(\n        tensor=quantized_tensor.to(torch.int8), scale=S, zero_point=Z\n    )  # Return the quantized tensor, scale, and zero point\nThis is the only required change to the implementation. In this implementation, we choose the clipping values manually, but in production cases, the clipping values are usually computed from the data distribution via different methods, such as Percentile Clipping, or even optimization methods such as minimizing the KL Divergence between the original and dequantized distribution. This process is called Calibration.\n\n\n\nAnother method to improve the performance of the quantized model is called Quantization Granularity. With the above implementation, we are computing the scale and zero point for the entire tensor. We could improve on this, by computing these values for a sub-part of the tensor. There are different variants of tensor splitting, such as Per Channel, Per Token, etc. The only difference between these variants is across which dimension is the zero point and scale computed. This can further lead to a better performance of the model, with cost of only a few extra bytes in memory. To save time and space, we will not be implementing these methods from scratch here, but just have this in mind when currently used quantization schemes are shown."
  },
  {
    "objectID": "articles/quantization-aware-training.html#quantization-aware-training-1",
    "href": "articles/quantization-aware-training.html#quantization-aware-training-1",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "With this out of the way, we can finally take a look at the concept of Quantization Aware Training. To further improve the performance of the quantized model at inference time, we can use the concept of Quantization Aware Training. This method involves making the model used to quantized weights, activations respectively. This involves training or fine-tuning the model with something called Fake Quantization. This is a method to simulate the quantization process during training. This is accomplished by doing the following:\n\nOriginal weights are stored in the original data type, such as float16.\nComputation is done in the original data type.\nAfter we load the weights, we quantize them and then dequantize back. This is done to simulate the loading of integer weights and their dequantization done during the inference.\nThe activations of the previous layer can be quantized, then dequantized back to get the values that the model would be using during the inference. This depends whether weâ€™re doing both activation and weight quantization, or only weight quantization.\nWe then backpropagate the gradient w.r.t. the dequantized weights, and update the original weights.\n\nYou might be wondering, why do we compute with the dequantized weights, but update the original weights? If you remember our implementation, to dequantize the tensor, we round the values to the nearest integer. This means that if the gradient w.r.t. the dequantized weights is used to update the dequantized weights, the change could be too small to be further visible in the integer representation, therefore we would lose the update information. We can think of this as passing the gradient to the original weights. This is called STE or Straight Through Estimator.\nItâ€™s easier to see this in a diagram:\n Figure 4: Visual representation of Quantization Aware Training. In this diagram, we can see that the weights are stored in the original data type, and are quantized and dequantized during the forward pass. This simulates the inference process, where the weights are loaded from memory and then dequantized. During the backward pass, we pass the gradient to the original weights, which are then updated."
  },
  {
    "objectID": "articles/quantization-aware-training.html#implementation-1",
    "href": "articles/quantization-aware-training.html#implementation-1",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "To properly implement this, we would like to replace all of the torch.nn.Linear layers with our own custom implementation. This custom implementation would involve the following:\n\nQuantizing the weights\nDequantizing the weights.\nComputing the forward pass with the dequantized weights.\nBackpropagating the error w.r.t. the dequantized weights.\nUpdating the original weights with the gradient.\n\nTo do this, we can register a custom autograd function in PyTorch.\nclass FakeQuantizeFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx: torch.autograd.function.FunctionCtx,\n        tensor: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        W_min, W_max = tensor.min(), tensor.max()\n        Q_min, Q_max = torch.iinfo(torch.int8).min, torch.iinfo(torch.int8).max\n        scale = (W_max - W_min) / (Q_max - Q_min)\n        zero_point = torch.round(-Q_min - (W_min / scale))\n\n        # Quantize\n        quantized = torch.round(tensor / scale) + zero_point\n        quantized = torch.clamp(quantized, Q_min, Q_max)\n\n        # Dequantize\n        dequantized = (quantized - zero_point) * scale\n\n        # Save mask for backward pass\n        mask = (quantized &gt;= Q_min) & (quantized &lt;= Q_max)\n        ctx.save_for_backward(mask)\n\n        return dequantized\n\n    @staticmethod\n    def backward(\n        ctx: torch.autograd.function.FunctionCtx, \n        grad_output: torch.Tensor\n    ) -&gt; tuple[torch.Tensor]:\n        (mask,) = ctx.saved_tensors\n        return grad_output * mask\nThis function is used to simulate the quantization process during training. Note the mask variable, this is used to store a boolean mask of values that werenâ€™t clipped during the forward pass. Therefore, values that were clipped are not updated in the backward pass. This helps simulate the inference process and stabilizes the training process.\nAfter this, we can create a torch.nn.Module that encapsulates the FakeQuantizeFunction and replace all of the torch.nn.Linear layers with this module. We can do this using a simple utility function.\nclass QuantizedLinear(torch.nn.Linear):\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        # Quantize weights\n        quantized_weight = FakeQuantizeFunction.apply(self.weight)\n\n        # Use quantized weights for the linear operation\n        return torch.nn.functional.linear(input, quantized_weight, self.bias)\ndef replace_layers_with_quantized(model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"Replaces all Linear layers with QuantizedLinear layers\"\"\"\n    for name, module in model.named_children():\n        if isinstance(module, torch.nn.Linear):\n            setattr(model, name, QuantizedLinear(\n                in_features=module.in_features,\n                out_features=module.out_features,\n                bias=module.bias is not None\n            ))\n            # Copy the weights and bias\n            getattr(model, name).weight.data = module.weight.data\n            if module.bias is not None:\n                getattr(model, name).bias.data = module.bias.data\n        else:\n            replace_layers_with_quantized(module)\n    return model\nNow, we can create our own model and use this utility function to do QAT.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(784, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 10)\n)\n\nmodel = replace_layers_with_quantized(model)\n\n# Now we can train the model as usual\n# ..."
  },
  {
    "objectID": "articles/quantization-aware-training.html#how-to-do-this-in-practice",
    "href": "articles/quantization-aware-training.html#how-to-do-this-in-practice",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "As you might have noticed, we have written quite a lot of code, which doesnâ€™t handle a lot of edge cases and is rather simple. We can use PyTorchâ€™s AO library to do this for us. This library provides a QAT module, which provides functionality for different quantization schemes.\nThe examples shown were purely educational. In production, different quantization schemes are used to improve the performance of the model. AO currently provides 2 different quantization schemes (note the different granularities of quantization, as discussed in the Quantization Granularity section), which are:\n\nint8 per token dynamic activation quantization with int4 per group weight quantization: This method quantizes weights to int8 and activations to int4. Then, computation is done in original data type, that is float16 usually. This is a good starting point for quantization aware training.\nint4 per group weight quantization: This method quantizes weights to int4, but keeps the activations in float16. Then, weights are dequantized on the fly during the matmul kernel call. This is just to optimize the latency and performance of the model.\n\nTo reproduce our example with AO, you can use the following code:\nimport torch\nfrom torchao.quantization.qat import Int8DynActInt4WeightQATQuantizer\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(784, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 10)\n).cuda()\n\n# Quantizer for int8 dynamic per token activations +\n# int4 grouped per channel weights, only for linear layers\nqat_quantizer = Int8DynActInt4WeightQATQuantizer()\n\n# Insert \"fake quantize\" operations into linear layers.\nmodel = qat_quantizer.prepare(model)\n\n# Now we can train the model as usual\n# ...\nRecalling our example, we have replaced the torch.nn.Linear layers with our own custom implementation. Equivalent of this is done by the qat_quantizer.prepare(model) method. This method inserts the FakeQuantizeFunction into the linear layers, and replaces the weights in the matrix multiplication with the quantized weights.\nHowever, in case of inference, we do not want to do these steps. We only need to dequantize the weights and do the computation, as the weights are already quantized when loaded from memory. We havenâ€™t implemented this in our example, as its not relevant to the concept, but is required in production setting. AO provides a qat_quantizer.convert(model) method, which does this for us. We can use the following code to achieve this:\n# Convert the model to the quantized model\n# This replaces all the \"fake quantize\" operations with the actual quantize operations\nmodel = qat_quantizer.convert(model)\n\n# Now we can use the model for inference\n# ..."
  },
  {
    "objectID": "articles/quantization-aware-training.html#tips-and-tricks",
    "href": "articles/quantization-aware-training.html#tips-and-tricks",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "Finetuning: finetuning the model with QAT is usually a better approach then training the model from scratch.\nLayers: quantizing only some layers is a good approach, some layers are influenced more by the quantization process. Try experimenting with replacing only some layers. Replacing the later layers is usually better than replacing the earlier layers. Also, in general itâ€™s not a good approach to quantize critical layers, such as attention. A good approach is to quantize the feed-forward layers, as those are the ones that require the most memory."
  },
  {
    "objectID": "articles/quantization-aware-training.html#conclusion",
    "href": "articles/quantization-aware-training.html#conclusion",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "In this tutorial, we have looked at the concept of quantization aware training, how does it work in depth, its benefits and how to implement it from scratch in PyTorch, and how to use AO to do this for us, which is a lot more efficient approach."
  }
]