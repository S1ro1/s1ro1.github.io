[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My name is Matej, but everyone calls me Siro. I’m a student of Artificial Intelligence and Machine Learning @ Brno University of Technology.\nI work at 🤗 Hugging Face, where you can mostly find me working on distributed training and inference in accelerate and transformers. I’m also one of the core maintainers of Cluster Bot.\nAs you might have guessed, I’m mostly interested in AI and where it intersects with systems programming. I liked AI before, but I was getting more and more annoyed by waiting for my training runs to finish, so I switched to making the runs go brrr 🚀.\n\n\nWhen I feel like it, I like to write about random topics from the world of AI and HPC. If any of this interests you, I believe you can find something useful below. Anything from chip design to distributed training…"
  },
  {
    "objectID": "index.html#articles",
    "href": "index.html#articles",
    "title": "About",
    "section": "",
    "text": "When I feel like it, I like to write about random topics from the world of AI and HPC. If any of this interests you, I believe you can find something useful below. Anything from chip design to distributed training…"
  },
  {
    "objectID": "articles/deepcompile.html",
    "href": "articles/deepcompile.html",
    "title": "DeepCompile",
    "section": "",
    "text": "In release 0.16.6 of DeepSpeed, you can find a new interesting feature. It’s called DeepCompile, in the release notes it’s hidden as the last feature (who reads those, right?) and if friend didn’t tag me with DeepSpeed’s post on X, I would have probably missed it. So why is it interesting?\nDeepSpeed is a framework for distributed training, it’s a pretty big library, most notable feature being probably the implementation of ZeRO. With DeepCompile, they enable compiler-based optimizations for ZeRO algorithms. Why should we care you might ask? There currently is a lot of work going on in the field of optimizing distributed training. The common problem is that communication operations (all-gather, reduce-scatter, etc.) which are ever-present in the training loop, are inserted at runtime, making them hard to optimize.\n\n\n\n\n\n\nNote\n\n\n\nIn the rest of this article, I’ll assume you are familiar with communication primitives and basics of distributed training."
  },
  {
    "objectID": "articles/deepcompile.html#zero-training-loop",
    "href": "articles/deepcompile.html#zero-training-loop",
    "title": "DeepCompile",
    "section": "ZeRO training loop",
    "text": "ZeRO training loop\nI mentioned that each GPU holds only a partition of the model, optimizer state and gradients. But how can the training loop work then? Before a part of the model is used (its .forward() method is called), it is all-gathered across all GPUs. Then after its forward pass, the memory is freed (usually, though this can be controlled by the user). When this layer is again needed for backward pass, it’s all-gathered again (if it was freed before). Then at the optimizer step, each GPU is responsible for updating only its partition of the model.\n Figure 1: Visualization of the ZeRO training loop showing how model parameters are partitioned across GPUs, all-gathered when needed for forward/backward passes, and then freed to conserve memory."
  },
  {
    "objectID": "articles/deepcompile.html#prefetching",
    "href": "articles/deepcompile.html#prefetching",
    "title": "DeepCompile",
    "section": "Prefetching",
    "text": "Prefetching\nIf you’re already familiar with this, or curious enough, you might have identified part of the problem. GPUs are very good at overlapping communication and computation. This allows us to start the all-gather for the current layer while the previous layer is still present in the GPU memory. Why can’t we start even earlier, beginning the all-gather operation before the pre-previous layer? We can! This method is called prefetching. But how do we know when to stop/start the prefetching? If we start too early, we risk having too many layers in the memory -&gt; RuntimeError: CUDA error: out of memory. If we start too late, the layer might not be gathered in time for its forward pass -&gt; slow training 💤. PyTorch implementation addresses this by prefetching maximum of 1 layer ahead, therefore maximum of 2 layers are in memory at the same time. But this is only a somewhat conservative heuristic. We can do better (and DeepCompile does exactly that, more on that later)."
  },
  {
    "objectID": "articles/deepcompile.html#resharding",
    "href": "articles/deepcompile.html#resharding",
    "title": "DeepCompile",
    "section": "Resharding",
    "text": "Resharding\nIf we have enough memory left, do we need to free the memory after the forward pass of the layer? No, we don’t! PyTorch controls this via options like ShardingStrategy.SHARD_GRAD_OP for FSDP1 and reshard_after_forward=False for FSDP2. This way, the layer is kept in memory after its foward pass until it’s again needed for backward. But this is again a binary choice, either we keep the layer in memory or we don’t. We can do better. Again, DeepCompile does!"
  },
  {
    "objectID": "articles/deepcompile.html#offloading",
    "href": "articles/deepcompile.html#offloading",
    "title": "DeepCompile",
    "section": "Offloading",
    "text": "Offloading\nSome memory parts, like the optimizer state, are not needed for the training loop. The optimizer state is only needed for the parameter update (after the backward pass). Do we need to keep it in GPU memory? No, we don’t. But how do we know how much we can offload? Offloading too much takes too long and offloading too little results in few memory savings. There again isn’t a good general solution. DeepCompile has one."
  },
  {
    "objectID": "articles/deepcompile.html#optimization-passes",
    "href": "articles/deepcompile.html#optimization-passes",
    "title": "DeepCompile",
    "section": "Optimization passes",
    "text": "Optimization passes\n\nProactive Prefetching In the Prefetching section, we discussed how all-gather operations can be initialized before the layer is used. This pass does exactly that. Based on the memory usage from the profiler, it attempts to schedule the all-gathers operations as early as possible, while respecting the memory limit. So how would this look in the diagram?\n\n\n\n\nProactive Prefetching\n\n\nFigure 3: Proactive prefetching: The green line represents the new memory usage, it is higher than the original one, but stays below the memory limit. You can see that we prefetch as early as possible, maximizing the memory, but staying below the limit. I think it’s easier to understand this in code, so here’s how it could be implemented:\ndef proactive_prefetching_pass(graph):\n    unscheduled_all_gathers = []\n    new_graph = Graph()\n\n    for node in reversed(graph):\n        if not isinstance(node, AllGather): \n            # if it's not an all-gather, add to the new graph\n            new_graph.append(node)\n        else:\n            # Get memory requirements for all unscheduled all gathers\n            unscheduled_mem_usage = sum([\n                node.memory_usage for node in unscheduled_all_gathers\n            ]) + node.memory_usage # current all gather\n\n            # check if current all gather still fits into the memory limit\n            # profiler.current_memory_usage returns the memory usage at the current step\n            total_required_mem = (  \n                unscheduled_mem_usage + profiler.current_memory_usage(node)\n            ) \n            if total_required_mem &lt; memory_limit:\n                # if it fits, add to the unscheduled all gathers\n                unscheduled_all_gathers.append(node)\n            else:\n                # if it doesn't fit, schedule the current all gather\n                scheduled_all_gathers = fuse(unscheduled_all_gathers)\n                new_graph.append(scheduled_all_gathers)\n                unscheduled_all_gathers = [node]\n\n    # schedule the last unscheduled all gathers\n    last_scheduled_all_gathers = fuse(unscheduled_all_gathers)\n    new_graph.append(last_scheduled_all_gathers)\n    return new_graph\nAlgorithm 1: Proactive Prefetching\nThis pass traverses the graph in reverse order, collects all-gather operations that fit within the memory limit, and as adding another would exceed the memory limit, fuses them together and schedules them, effectively moving them as early as possible. You can notice a fuse operation we haven’t talked about. This function fuses multiple all-gather operations into a so-called bucket. This is done because communication operations involving small data sizes can be inefficient.\n\nSelective Resharding In the Resharding section, we discussed how we can keep the layer in memory after using it in forward, then reusing it in backward. This pass does exactly that. If there is still some memory left after applying the previous pass, we can use this available memory to keep some layers unsharded (i.e. not freeing the memory after the forward pass). We decide on which layers to keep, using the following heuristic:\n\n\\[\n\\frac{\\text{communication\\_time}_{op_i}}{\\text{data\\_size}_{op_i}}\n\\] , where \\(op_i\\) is the i-th operation in the graph and \\(\\text{communication\\_time}_{op_i}\\) is the time taken for its communication and \\(\\text{data\\_size}_{op_i}\\) is the size of the data communicated. The goal is to esentially maximize the communication time saved, while minimizing the memory usage.\n\nAdaptive Offloading\n\nAgain, this pass is very simple, DeepCompile basically detects if we’re lacking memory, and if so, it partially offloads the optimizer state to the CPU. When sufficient memory becomes available again, it begins reloading the state back to the GPU. This way, we don’t go OOM, however, we avoid unecessary time spent transferring data to the CPU and back."
  },
  {
    "objectID": "articles/transformers-tp.html",
    "href": "articles/transformers-tp.html",
    "title": "Tensor Parallelism in Transformers",
    "section": "",
    "text": "With the advent of large language (and now multi-modal) models, there is a growing need for efficient ways to train and serve these models. Models with 100s of billions of parameters are not uncommon and those do not fit into a single GPU. Tensor parallelism is one of the techniques that can be used to help with this problem. This article will cover the basics of tensor parallelism, how it works and most importantly, how you can use predefined tensor parallelism APIs in 🤗 Transformers."
  },
  {
    "objectID": "articles/transformers-tp.html#introduction",
    "href": "articles/transformers-tp.html#introduction",
    "title": "Tensor Parallelism in Transformers",
    "section": "",
    "text": "With the advent of large language (and now multi-modal) models, there is a growing need for efficient ways to train and serve these models. Models with 100s of billions of parameters are not uncommon and those do not fit into a single GPU. Tensor parallelism is one of the techniques that can be used to help with this problem. This article will cover the basics of tensor parallelism, how it works and most importantly, how you can use predefined tensor parallelism APIs in 🤗 Transformers."
  },
  {
    "objectID": "articles/transformers-tp.html#a.-how-does-tensor-parallelism-work",
    "href": "articles/transformers-tp.html#a.-how-does-tensor-parallelism-work",
    "title": "Tensor Parallelism in Transformers",
    "section": "2a. How does tensor parallelism work?",
    "text": "2a. How does tensor parallelism work?\nAt its core, tensor parallelism involves partitioning the model’s weights across multiple devices. Let’s consider a simple matrix multiplication, \\(Y = XW\\), which is a fundamental operation in neural networks. We will be considering common transformer architectures, so the weight matrix \\(W\\) is of shape \\(d\\_model \\times d\\_model\\) (For simplicity, we will assume square weight matrices, but this is not a requirement for tensor parallelism and doesn’t change anything) and the input matrix \\(X\\) is of shape \\(batch\\_size \\times d\\_model \\times seq\\_len\\). Again, we can ignore the batch size for now, as that will only cause duplication of the following operations across its dimension.\nWe can express the shapes as follows: \\[\nW \\in \\mathbb{R}^{d\\_model \\times d\\_model}\n\\]\n\\[\nX \\in \\mathbb{R}^{batch\\_size \\times d\\_model \\times seq\\_len}\n\\]\n\\[\nY \\in \\mathbb{R}^{batch\\_size \\times d\\_model \\times seq\\_len}\n\\]\n\nColumn-wise Partitioning\n\nThe weight matrix \\(W\\) is split column-wise across two GPUs, where \\(W_1\\) resides on GPU 1 and \\(W_2\\) on GPU 2. This doesn’t require any communication, as it is done at initialization. \\[\nW = \\begin{bmatrix} W_1 & W_2 \\end{bmatrix}\n\\]\n\n\\[\nW_1, W_2 \\in \\mathbb{R}^{d\\_model \\times \\frac{d\\_model}{2}}\n\\]\n\nThe input matrix \\(X\\) is broadcast to both GPUs (each GPU gets a full copy of \\(X\\)). (Broadcast operation)\n\n\\[\nY = XW = X \\begin{bmatrix} W_1 & W_2 \\end{bmatrix} = \\begin{bmatrix} XW_1 & XW_2 \\end{bmatrix} = \\begin{bmatrix} Y_1 & Y_2 \\end{bmatrix}\n\\]\n\nEach GPU computes a part of the output matrix \\(Y\\).\n\n\\[\nY_1 = XW_1 \\quad \\text{and} \\quad Y_2 = XW_2\n\\]\n\\[\nY_1, Y_2 \\in \\mathbb{R}^{batch\\_size \\times d\\_model \\times \\frac{seq\\_len}{2}}\n\\]\n\nThe final result \\(Y\\) is obtained by concatenating \\(Y_1\\) and \\(Y_2\\) along the column dimension. (AllGather operation)\n\n\\[\nY = \\begin{bmatrix} Y_1 & Y_2 \\end{bmatrix}\n\\]\n\n\nRow-wise Partitioning\n\nWe split the input matrix \\(X\\) column-wise across two GPUs, where \\(X_1\\) resides on GPU 1 and \\(X_2\\) on GPU 2. (Scatter operation)\n\n\\[\nX = \\begin{bmatrix} X_1 & X_2 \\end{bmatrix}\n\\]\n\\[\nX_1, X_2 \\in \\mathbb{R}^{batch\\_size \\times seq\\_len \\times \\frac{d\\_model}{2}}\n\\]\n\nThe weight matrix \\(W\\) is split row-wise, where \\(W_1\\) resides on GPU 1 and \\(W_2\\) on GPU 2. Again, this doesn’t require any communication, as each GPU only gets its part of the weight matrix at initialization.\n\n\\[\nW = \\begin{bmatrix} W_1 \\\\ W_2 \\end{bmatrix}\n\\]\n\\[\nW_1, W_2 \\in \\mathbb{R}^{\\frac{d\\_model}{2} \\times d\\_model}\n\\]\n\nEach GPU computes a part of the output matrix \\(Y\\).\n\n\\[\nY_1 = X_1W_1 \\quad \\text{and} \\quad Y_2 = X_2W_2\n\\]\n\\[\nY_1, Y_2 \\in \\mathbb{R}^{batch\\_size \\times seq\\_len \\times d\\_model}\n\\]\n\nThe final result \\(Y\\) is obtained by element-wise addition of \\(Y_1\\) and \\(Y_2\\). (AllReduce operation)\n\n\\[\nY = Y_1 + Y_2\n\\]\n#TODO Visualization\n\n\nBias\nBias is a vector of shape \\(d\\_model\\) and is added to the output after the matrix multiplication. You could notice that the previous equations don’t consider bias at all. Handling bias is pretty simple, we just have to do the following:\n\nColumn-wise partitioning: For column-wise partitioning, \\(Y_1\\) and \\(Y_2\\) were half the size of the original shape, therefore bias is split across GPUs, each GPU gets a vector \\(B_i \\in \\mathbb{R}^{\\frac{d\\_model}{2}}\\)\nRow-wise partitioning: Recall that \\(Y_1\\) and \\(Y_2\\) were already the original shape, therefore bias is replicated across GPUs, each GPU gets a vector \\(B_i \\in \\mathbb{R}^{d\\_model}\\) (each GPU gets a full copy of the bias)\n\n\n\nApplying tensor parallelism\nColumn-wise partitioning compliments row-wise partitioning very well. Remember that row-wise partitioning requires the input to split column-wise, which is exactly the output of column-wise partitioning. This way, concatenation of \\(Y_1\\) and \\(Y_2\\) after the multiplication step of column-wise partitioning is not requireed - saving communication. After this, row-wise can be applied to each of the shards from the previous step, only requiring weights to be split across GPUs.\n#TODO Visualization"
  },
  {
    "objectID": "articles/transformers-tp.html#tensor-parallelism-for-transformer-models",
    "href": "articles/transformers-tp.html#tensor-parallelism-for-transformer-models",
    "title": "Tensor Parallelism in Transformers",
    "section": "Tensor Parallelism for transformer models",
    "text": "Tensor Parallelism for transformer models\nAverage transformer architecture consists of multiple layers, each containing a self-attention and a feed-forward network. We would like to apply tensor parallelism to each of these. As mentioned earlier, we can apply column-wise partitioning first, then row-wise partitioning.\n\nFeed-Forward Network\nApplying tensor parallelism to feed-forward network is straightforward. As discussed ealier, column-wise partitioning followed by row-wise partitioning is a natural fit and is really easy to apply to Feed-Forward Network. Do not mistake \\(W1\\) and \\(W2\\) for \\(W_1\\) and \\(W_2\\) from the previous sections, \\(W1\\) and \\(W2\\) represent two distinct linear layers, not the same weight matrix split across GPUs.\n\\[\nY = \\text{FFN}(X) = \\text{ReLU}(XW1)W2\n\\]\n#TODO Visualization\n\n\nSelf-Attention\nSelf-attention might seem a bit tricky at first, though it is actually quite simple and natural. We split attention column-wise, making sure each GPU gets a single attention head. Then we split the output projection row-wise.\n\\[\nY = \\text{Dropout}(\\text{Self-Attention}(X)W_O)\n\\]\n#TODO Visualization"
  },
  {
    "objectID": "articles/transformers-tp.html#b.-sequence-parallelism",
    "href": "articles/transformers-tp.html#b.-sequence-parallelism",
    "title": "Tensor Parallelism in Transformers",
    "section": "2b. Sequence Parallelism",
    "text": "2b. Sequence Parallelism\nWe will be talking about sequence parallelism as a parallelism technique used together with tensor parallelism, on layers that classical tensor parallelism doesn’t apply to. In particular to LayerNorms and Dropouts. These 2 layers, require full activations to be present for them to be numerically correct.\nAs we know, both of these operations require the access to the whole hidden dimension to be applied. We can overcome this by splitting the activations across sequence dimension - henceforth sequence parallelism. In practice, we apply tensor parallelism on layers that aren’t affected by this problem (FFN + Self-Attention), then apply sequence parallelism on the following layers (LayerNorm + Dropout) and we repeat this across all layers, alternating between tensor and sequence parallelism.\nThis introduces some extra subtleties in communication, as we need to ensure correct transition between tensor and sequence parallelism regions.\n\nTensor Parallel -&gt; Sequence Parallel: Previously, AllReduce was used on the output of row-wise partition. Now, the outputs also have to be split across GPUs across the sequence dimension after being reduced. This operation is called ReduceScatter.\nSequence Parallel -&gt; Tensor Parallel: Each GPU has only part of the activations after the sequence parallel region, meaning it only has a fraction of the tokens (hidden dimension is full). The following tensor parallel region requires full copy of the activations, so an AllGather operation is required, construct the full activations on all GPUs again.\n\n\nSome caveats to note\n\nTensor parallelism requires a lot of communication between GPUs, that’s why it’s usually not used inter-node, but only intra-node, taking advantage of high bandwidth interconnects.\nBias requires extra handling, different for row-wise vs column-wise partitioning.\nTo minimize the communication overhead, column-wise partitioning is applied first, then row-wise partitioning, this way we minimize the amount of data that needs to be sent between GPUs making the approach more efficient.\nCommonly used in conjuction with Sequence Parallelism, as they complement each other very well.\nCan also be used on model embeddings, where row-wise partitioning across vocabulary size is applied."
  },
  {
    "objectID": "articles/transformers-tp.html#distributed-pytorch-basics",
    "href": "articles/transformers-tp.html#distributed-pytorch-basics",
    "title": "Tensor Parallelism in Transformers",
    "section": "3. Distributed PyTorch basics",
    "text": "3. Distributed PyTorch basics\nAs with almost everything these days, PyTorch is the library of choice for this feature. In this section, we will cover some basics of distributed PyTorch and some features you should be familiar with, as they make this all possible. If you know what torch.distributed.ProcessGroup, torch.distributed.DeviceMesh and torch.distributed.DTensor are, you can skip this section and go to the next one.\nPyTorch provides a distributed package that allows you to run PyTorch code across multiple processes, even across multiple machines. In this section, we’ll consider that each process is running on a single GPU, all processes are running on the same machine.\n\nProcess Group\nEncapsulating these processes is a torch.distributed.ProcessGroup abstraction. This class ensures communication and synchronization between processes. You should setup the process group, each time you want to run a distributed code.\nrank = int(os.environ[\"RANK\"]) # get the rank of the current process, env variable is set by `torchrun`\ndevice = torch.device(f\"cuda:{rank}\")\ntorch.cuda.set_device(device) # set the device of the current process\ntorch.distributed.init_process_group(backend=\"nccl\", device_id=device) # initialize the process group with a communication backend\nThis is a snippet to setup the process group for a single machine with multiple GPUs. You need to run this code, using a torchrun command, as such:\ntorchrun --nproc_per_node=8 main.py # This will run the script with 8 processes, each on a different GPU\ntorchrun is a abbrevation of torch.distributed.run, so you can also use it as:\npython3 -m torch.distributed.run --nproc_per_node=8 main.py\nThis process group should also be destroyed after the script is finished, as it’s not needed anymore.\nif torch.distributed.is_initialized():\n    torch.distributed.destroy_process_group()\n\n\nDevice Mesh\nPyTorch provides another handy abstraction, torch.distributed.DeviceMesh. For tensor parallelism, DeviceMesh isn’t as useful, but it’s good to be familiar with it. It’s mostly useful when combining multiple parallelization techniques (more on that in another article). It enables creating multiple sub-meshes on a single ProcessGroup, where each sub-mesh is responsible for a different parallelization technique. In Transformers, we do the initalization for you, so you don’t need to worry about it. For tensor parallelism, it is created like this:\ndevice_mesh = torch.distributed.device_mesh.init_device_mesh(\"cuda\", (8, ), mesh_dim_names=(\"tp, ))\nThen we can use this device mesh to split the model across GPUs and PyTorch will know how to do the communication.\n\n\nDTensor\nLast but least, we have torch.distributed.DTensor. Abbrevation for Distributed Tensor, it’s a tensor subclass that on-top of the usual tensor operations, also handles the distributed logic. You won’t be interacting with it directly, but again, it’s a class that whole process of tensor parallelism is built on top of. Most important part of DTensor that is crucial to understand is the placement attribute. It’s an attribute that tells PyTorch how is the tensor placed on the devices. It’s an enum with the following possible values:\n\nShard(dimension) - Annotates that this DTensor is sharded across a given dimension, over the device mesh it was constructed under. For example, if we would like to shard weights for column-wise partitioning, we would do:\n\nweight = ...\nweight = DTensor.from_local(weight, device_mesh[\"tp\"], placements=[Shard(0)]) # Shard across the 1st (column-wise) dimension\nbias = ...\nbias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Shard(-1)]) # Shard across the ONLY dimension\nTo give another example, for row-wise partitioning, we would do:\nweight = ...\nweight = DTensor.from_local(weight, device_mesh[\"tp\"], placements=[Shard(1)]) # Shard across the 2nd (row-wise) dimension\nbias = ...\nbias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Replicate()]) # Replicate bias across all GPUs\n\nReplicate() - Annotates that this DTensor is replicated across all devices. Pretty straight-forward, it only creates a full copy of the tensor on each device.\nPartial() - This placement is mostly of no interest to us, it’s used to annotate that this tensor is pending a reduction operation.\n\nYou can find definitions for all of the different partitioning schemes in this file. But wait, why do we use different sharding dimensions for column-wise and row-wise? It’s because it’s applied on nn.Linear weights, where the operation is \\(Y = XW^T + b\\). Math, as specified before, applies the same."
  },
  {
    "objectID": "articles/transformers-tp.html#using-transformers",
    "href": "articles/transformers-tp.html#using-transformers",
    "title": "Tensor Parallelism in Transformers",
    "section": "4. Using 🤗 Transformers",
    "text": "4. Using 🤗 Transformers\nAll of the methods mentioned above seem to be a lot of work to implement manually. Thankfully, 🤗 Transformers have got you covered! These methods are implemented in tensor_parallel.py, with shard_and_distribute_module being the entrypoint, to prepare a single module with tensor parallelism pre-configured. We provide multiple different paritioning strategies, so let’s take a look at them.\n\nColwiseParallel - A simple column-wise partitioning, being able to handle both weights and biases, does exactly what we’ve discussed before.\nRowwiseParallel - Again, row-wise partitioning as dicussed before, supports weights and biases, on top of that it also supports nn.Embedding modules.\nSequenceParallel - Sequence parallel implementation, for support of LayerNorm and Dropout layers. Also supports Python implementation of RMSNorm (see this)\nPackedColwiseParallel - A variant of column-wise partitioning, however it works on packed weights (i.e. up_proj and gate_proj being packed together). For more details, see this comment\nPackedRowwiseParallel - A variant of row-wise partitioning, works on packed weights, for more details check the comment linked above.\nGatherParallel - A very simple class, that only makes the outputs of the module to be gathered across devices.\nIsolatedParallel - This is a special case, where we want to isolate the module from the rest of the devices (world). This is used for Experts in MoE layers, basically creating Expert parallelism of sorts.\n\n\n\n\n\n\n\nTechnical implementation details\n\n\n\n\n\nFor readers interested in the details, it’s actually quite simple to understand how this works. Let’s take a look at the ColwiseParallel class.\nclass ColwiseParallel(TensorParallelLayer):\n    def __init__(\n        self,\n        *,\n        input_layouts: Optional[Placement] = None,\n        output_layouts: Optional[Placement] = None,\n        use_local_output: bool = True,\n        use_dtensor=True,\n    ):\n        self.input_layouts = (input_layouts or Replicate(),) # The input sharding coming from the previous layer\n        self.output_layouts = (output_layouts or Shard(-1),) # Desired output sharding\n        self.desired_input_layouts = (Replicate(),) # Desired input sharding, inputs should be replicated across GPUs (Broadcast operation)\n        self.use_local_output = use_local_output # True\n        self.use_dtensor = use_dtensor # True\nIn the __init__ method, we define these attributes, where input_layouts and output_layouts describing, how the input and output tensors should be placed on the devices. desired_input_layouts is used to specify, how the input SHOULD be placed on the devices. Then, 2 methods are defined: _prepare_input_fn and _prepare_output_fn. As you might have guessed, these methods are used to re-distribute the inputs/outputs to the desired layout. This is done via DTensors redistribute method, where we can specify the desired placement.\n@staticmethod\ndef _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n    ...\n    return inputs.redistribute(placements=desired_input_layouts, device_mesh=device_mesh)\n@staticmethod\ndef _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n    ...\n    return outputs.redistribute(placements=output_layouts, device_mesh=device_mesh)\nThese methods are then applied as hooks to the module, using PyTorch’s torch.distributed.tensor.distribute_module function (we have our own implementation that is slightly more efficient, but works the same way). It basically registers _prepare_input_fn as a pre-forward hook and _prepare_output_fn as a forward hook.\ndistribute_module(\n    module,\n    device_mesh,\n    partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),\n    partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),\n)\nThen, one more important method is required: partition_tensor. This method is used to partition the module’s weights or biases according to the desired partitioning strategy. This implementation does a bit more stuff than just partitioning, being used in a lot of places, but it’s main idea is to get the corresponding shard of the parameter, create a DTensor from it and return a nn.Parameter object. See the implementation here for details.\n\n\n\n\n🤗 Transformers Tensor Parallelism\nfrom transformers import AutoModelForCausalLM\n\n# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" # uncomment for smaller number of GPUs\nmodel_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" # better to visualize\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=\"auto)\n\nprint(model._tp_plan)\nThis will load the model with tensor parallelism pre-configured for the best performance. The model has an attribute _tp_plan which contains information about the tensor parallelism plan, running the above with\ntorchrun --nproc_per_node=8 main.py\nwill give a result such as:\n{\n    \"layer.*.self_attn.q_proj\": \"colwise\",\n    \"layer.*.self_attn.k_proj\": \"colwise\",\n    \"layer.*.self_attn.v_proj\": \"colwise\",\n    \"layer.*.self_attn.o_proj\": \"rowwise\",\n    ...\n}\nThis tells us that the query projection is partitioned column-wise, the key and value projections are partitioned column-wise and the output projection is partitioned row-wise. The model is directly loaded and configured with this plan.\n\n\nTODO\n\nShould we first implement it in a stable API or just showcase the current way to do it (setting model.config.tp_plan and supports_tp)?\nSupported models\nTraining/Inference (Accelerate for training)"
  },
  {
    "objectID": "articles/quantization-aware-training.html",
    "href": "articles/quantization-aware-training.html",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "In this tutorial, we will be looking at the concept of quantization aware training, how does it work in depth, its benefits and how to implement it in PyTorch. To properly understand this concept, proper understanding of quantization basics is required.\n\n\nFormally, quantization is the process of constraining an input from a continuous or otherwise large set of values to a discrete set of values. You can think of it as a way to reduce the precision of the data. In neural networks, quantization is the process of reducing the precision of the weights and activations. This can be helpful in different ways.\n\nMemory Reduction: In the example of current LLMs, the weights of the feed-forward layers are quite large. Imagine a forward layer weight matrix in Llama 3 70B Model, the weight matrix could be of size 8192 * 8192. In case of float16, this weight matrix would require 8192 * 8192 * 2 = 134,217,728 bytes of memory (approximately 128 MB). This is a lot of memory to store and process, when we consider the fact that the model has multiple such layers. In case we reduced the precision of the weights, we can reduce the load times from memory approximately two, four-fold respectively when using int8 or int4 data types.\nSpeedup: Quantization can also help in speeding up the inference, sometimes even the training process. From computer architecture perspective, the operations on large data types, such as float16 or float32 are expensive and slow. These operations are way faster and cheaper when performed on smaller data types like int8 or float8. When we take a look at the current state-of-the-art GPU Nvidia H100 and its datasheet, we can see that the performance of the GPU Tensor Cores linearly increases with the decrease in the data type size.\n\nNow, that we have a basic understanding on why quantization is important, let’s take a look at how quantization works.\nImportant To simplify things, we will only be looking at quantization to a lower precision data type, that exists in the PyTorch framework, to avoid hassles of binary operations. That is, from torch.float16 to torch.int8. Also, we will be considering a method called Linear Quantization. This is the most common method of quantization.\n\n\n\nIf we think of int8 as a data type, it can store values in the range of [-128, 127]. However, our weights and activations in float16 have a range of [-65504, 65504]. Also, this range in float16 is not uniformly distributed, therefore accommodating a lot more possible values. To quantize the weights and activations, we need to map the values in float16 to the range of int8. This can be done by the following steps:\n\nMin and Max Calculation: We need to calculate the minimum and maximum values in the data. This will tell us what values map to int8.min and int8.max.\n\n Figure 1: Visual representation of min and max calculation, W_max and W_min are the maximum and minimum values in tensor to be quantized, these values then map to int8.max and int8.min respectively.\n\nZero Point Calculation: We can think of zero point as the point where the float16 value of 0 lies in the int8 data type. This basically maps the real number r=0 to a quantized integer.\n\n Figure 2: Visual representation of zero point calculation. Z on the quantized axis is the zero point, and represents where the r=0.0 lies on the quantized axis.\n\nScale Calculation: The scale basically tells us, how much each unit in the quantized data type represents in the original data type. Imagine a scale of 1.0, this means that each unit in the quantized data type represents 1.0 in the original data type. The larger the scale, the larger is the original input range.\n\nAfter these steps, we have everything we need to quantize and dequantize the data. With r being the real number, q being the quantized number, Z being the zero point, and S being the scale, the quantization and dequantization can be done by the following equations:\n\\[\nq = \\text{round}\\left(\\frac{r}{S}\\right) + Z\n\\]\n\\[\nr = (q - Z) \\cdot S\n\\]\nWith some additional math, we can also derive the scale and zero point equations from the min and max values.\n\\[\nS = \\frac{W_{max} - W_{min}}{Q_{max} - Q_{min}} = \\frac{W_{max} - W_{min}}{127 - (-128)}\n\\]\n\\[\nZ = \\text{round}\\left(Q_{min} - \\frac{W_{min}}{S}\\right) = \\text{round}\\left(-128 - \\frac{W_{min}}{S}\\right)\n\\]\n\n\n\nimport torch\nfrom collections import namedtuple\n\nQTensor = namedtuple(\n    \"QTensor\", [\"tensor\", \"scale\", \"zero_point\"]\n)  # we need to track the scale and zero point to dequantize the tensor later\n\n\ndef quantize_tensor(tensor: torch.Tensor) -&gt; QTensor:\n    W_min = tensor.min()\n    W_max = tensor.max()\n\n    Q_min = torch.iinfo(torch.int8).min  # Get the minimum value of the int8 data type\n    Q_max = torch.iinfo(torch.int8).max  # Get the maximum value of the int8 data type\n\n    S = (W_max - W_min) / (Q_max - Q_min)  # Calculate the scale\n    Z = torch.round(Q_min - (W_min / S))  # Calculate the zero point\n\n    quantized_tensor = torch.round(tensor / S) + Z  # Quantize the tensor\n\n    return QTensor(\n        tensor=quantized_tensor.to(torch.int8), scale=S, zero_point=Z\n    )  # Return the quantized tensor, scale, and zero point\n\n\ndef dequantize_tensor(q_tensor: QTensor) -&gt; torch.Tensor:\n    return (\n        q_tensor.tensor.to(torch.float16) - q_tensor.zero_point\n    ) * q_tensor.scale  # simply compute the real value from the already computed data\n\n\nYou might have noticed that we lose quite a lot of information while quantizing the tensor. This might lead to a precision loss, which can be detrimental to the performance of the model. Imagine a scenario where our input tensor distribution looks like the following:\n Figure 3: A pretty common distribution of weights, where most of the values are centered around 0.0.\nNow imagine, we have a single data-point, which is far from the distribution, let’s say W_max=1000.0. If we try to quantize this tensor, the scale would be very large, therefore a distance of 1 in the quantized data would represent a very large distance in the original unquantized data. But remember, our input tensor is distributed around 0.0, with most values lying in the range of [-10.0, 10.0]. This means that most of these values would be quantized to the same value, therefore losing a lot of information leading to a loss in performance.\nTo fix this issue, we can use a method called Clipping. This method involves clipping the values of the tensor to a certain range, and then quantizing the tensor. Our PyTorch implementation can be extended to include this method by the following:\ndef quantize_tensor(\n    tensor: torch.Tensor, clip_min: float | None = None, clip_max: float | None = None\n) -&gt; QTensor:\n    if clip_min or clip_max: # check if atleast one of the clip values is provided\n        tensor = torch.clamp(tensor, clip_min, clip_max)\n\n    W_min = tensor.min()\n    W_max = tensor.max()\n\n    Q_min = torch.iinfo(torch.int8).min  # Get the minimum value of the int8 data type\n    Q_max = torch.iinfo(torch.int8).max  # Get the maximum value of the int8 data type\n\n    S = (W_max - W_min) / (Q_max - Q_min)  # Calculate the scale\n    Z = torch.round(Q_min - (W_min / S))  # Calculate the zero point\n\n    quantized_tensor = torch.round(tensor / S) + Z  # Quantize the tensor\n\n    return QTensor(\n        tensor=quantized_tensor.to(torch.int8), scale=S, zero_point=Z\n    )  # Return the quantized tensor, scale, and zero point\nThis is the only required change to the implementation. In this implementation, we choose the clipping values manually, but in production cases, the clipping values are usually computed from the data distribution via different methods, such as Percentile Clipping, or even optimization methods such as minimizing the KL Divergence between the original and dequantized distribution. This process is called Calibration.\n\n\n\nAnother method to improve the performance of the quantized model is called Quantization Granularity. With the above implementation, we are computing the scale and zero point for the entire tensor. We could improve on this, by computing these values for a sub-part of the tensor. There are different variants of tensor splitting, such as Per Channel, Per Token, etc. The only difference between these variants is across which dimension is the zero point and scale computed. This can further lead to a better performance of the model, with cost of only a few extra bytes in memory. To save time and space, we will not be implementing these methods from scratch here, but just have this in mind when currently used quantization schemes are shown.\n\n\n\n\nWith this out of the way, we can finally take a look at the concept of Quantization Aware Training. To further improve the performance of the quantized model at inference time, we can use the concept of Quantization Aware Training. This method involves making the model used to quantized weights, activations respectively. This involves training or fine-tuning the model with something called Fake Quantization. This is a method to simulate the quantization process during training. This is accomplished by doing the following:\n\nOriginal weights are stored in the original data type, such as float16.\nComputation is done in the original data type.\nAfter we load the weights, we quantize them and then dequantize back. This is done to simulate the loading of integer weights and their dequantization done during the inference.\nThe activations of the previous layer can be quantized, then dequantized back to get the values that the model would be using during the inference. This depends whether we’re doing both activation and weight quantization, or only weight quantization.\nWe then backpropagate the gradient w.r.t. the dequantized weights, and update the original weights.\n\nYou might be wondering, why do we compute with the dequantized weights, but update the original weights? If you remember our implementation, to dequantize the tensor, we round the values to the nearest integer. This means that if the gradient w.r.t. the dequantized weights is used to update the dequantized weights, the change could be too small to be further visible in the integer representation, therefore we would lose the update information. We can think of this as passing the gradient to the original weights. This is called STE or Straight Through Estimator.\nIt’s easier to see this in a diagram:\n Figure 4: Visual representation of Quantization Aware Training. In this diagram, we can see that the weights are stored in the original data type, and are quantized and dequantized during the forward pass. This simulates the inference process, where the weights are loaded from memory and then dequantized. During the backward pass, we pass the gradient to the original weights, which are then updated.\n\n\n\nTo properly implement this, we would like to replace all of the torch.nn.Linear layers with our own custom implementation. This custom implementation would involve the following:\n\nQuantizing the weights\nDequantizing the weights.\nComputing the forward pass with the dequantized weights.\nBackpropagating the error w.r.t. the dequantized weights.\nUpdating the original weights with the gradient.\n\nTo do this, we can register a custom autograd function in PyTorch.\nclass FakeQuantizeFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx: torch.autograd.function.FunctionCtx,\n        tensor: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        W_min, W_max = tensor.min(), tensor.max()\n        Q_min, Q_max = torch.iinfo(torch.int8).min, torch.iinfo(torch.int8).max\n        scale = (W_max - W_min) / (Q_max - Q_min)\n        zero_point = torch.round(-Q_min - (W_min / scale))\n\n        # Quantize\n        quantized = torch.round(tensor / scale) + zero_point\n        quantized = torch.clamp(quantized, Q_min, Q_max)\n\n        # Dequantize\n        dequantized = (quantized - zero_point) * scale\n\n        # Save mask for backward pass\n        mask = (quantized &gt;= Q_min) & (quantized &lt;= Q_max)\n        ctx.save_for_backward(mask)\n\n        return dequantized\n\n    @staticmethod\n    def backward(\n        ctx: torch.autograd.function.FunctionCtx, \n        grad_output: torch.Tensor\n    ) -&gt; tuple[torch.Tensor]:\n        (mask,) = ctx.saved_tensors\n        return grad_output * mask\nThis function is used to simulate the quantization process during training. Note the mask variable, this is used to store a boolean mask of values that weren’t clipped during the forward pass. Therefore, values that were clipped are not updated in the backward pass. This helps simulate the inference process and stabilizes the training process.\nAfter this, we can create a torch.nn.Module that encapsulates the FakeQuantizeFunction and replace all of the torch.nn.Linear layers with this module. We can do this using a simple utility function.\nclass QuantizedLinear(torch.nn.Linear):\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        # Quantize weights\n        quantized_weight = FakeQuantizeFunction.apply(self.weight)\n\n        # Use quantized weights for the linear operation\n        return torch.nn.functional.linear(input, quantized_weight, self.bias)\ndef replace_layers_with_quantized(model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"Replaces all Linear layers with QuantizedLinear layers\"\"\"\n    for name, module in model.named_children():\n        if isinstance(module, torch.nn.Linear):\n            setattr(model, name, QuantizedLinear(\n                in_features=module.in_features,\n                out_features=module.out_features,\n                bias=module.bias is not None\n            ))\n            # Copy the weights and bias\n            getattr(model, name).weight.data = module.weight.data\n            if module.bias is not None:\n                getattr(model, name).bias.data = module.bias.data\n        else:\n            replace_layers_with_quantized(module)\n    return model\nNow, we can create our own model and use this utility function to do QAT.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(784, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 10)\n)\n\nmodel = replace_layers_with_quantized(model)\n\n# Now we can train the model as usual\n# ...\n\n\n\nAs you might have noticed, we have written quite a lot of code, which doesn’t handle a lot of edge cases and is rather simple. We can use PyTorch’s AO library to do this for us. This library provides a QAT module, which provides functionality for different quantization schemes.\nThe examples shown were purely educational. In production, different quantization schemes are used to improve the performance of the model. AO currently provides 2 different quantization schemes (note the different granularities of quantization, as discussed in the Quantization Granularity section), which are:\n\nint8 per token dynamic activation quantization with int4 per group weight quantization: This method quantizes weights to int8 and activations to int4. Then, computation is done in original data type, that is float16 usually. This is a good starting point for quantization aware training.\nint4 per group weight quantization: This method quantizes weights to int4, but keeps the activations in float16. Then, weights are dequantized on the fly during the matmul kernel call. This is just to optimize the latency and performance of the model.\n\nTo reproduce our example with AO, you can use the following code:\nimport torch\nfrom torchao.quantization.qat import Int8DynActInt4WeightQATQuantizer\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(784, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 10)\n).cuda()\n\n# Quantizer for int8 dynamic per token activations +\n# int4 grouped per channel weights, only for linear layers\nqat_quantizer = Int8DynActInt4WeightQATQuantizer()\n\n# Insert \"fake quantize\" operations into linear layers.\nmodel = qat_quantizer.prepare(model)\n\n# Now we can train the model as usual\n# ...\nRecalling our example, we have replaced the torch.nn.Linear layers with our own custom implementation. Equivalent of this is done by the qat_quantizer.prepare(model) method. This method inserts the FakeQuantizeFunction into the linear layers, and replaces the weights in the matrix multiplication with the quantized weights.\nHowever, in case of inference, we do not want to do these steps. We only need to dequantize the weights and do the computation, as the weights are already quantized when loaded from memory. We haven’t implemented this in our example, as its not relevant to the concept, but is required in production setting. AO provides a qat_quantizer.convert(model) method, which does this for us. We can use the following code to achieve this:\n# Convert the model to the quantized model\n# This replaces all the \"fake quantize\" operations with the actual quantize operations\nmodel = qat_quantizer.convert(model)\n\n# Now we can use the model for inference\n# ...\n\n\n\n\nFinetuning: finetuning the model with QAT is usually a better approach then training the model from scratch.\nLayers: quantizing only some layers is a good approach, some layers are influenced more by the quantization process. Try experimenting with replacing only some layers. Replacing the later layers is usually better than replacing the earlier layers. Also, in general it’s not a good approach to quantize critical layers, such as attention. A good approach is to quantize the feed-forward layers, as those are the ones that require the most memory.\n\n\n\n\nIn this tutorial, we have looked at the concept of quantization aware training, how does it work in depth, its benefits and how to implement it from scratch in PyTorch, and how to use AO to do this for us, which is a lot more efficient approach."
  },
  {
    "objectID": "articles/quantization-aware-training.html#what-is-quantization",
    "href": "articles/quantization-aware-training.html#what-is-quantization",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "Formally, quantization is the process of constraining an input from a continuous or otherwise large set of values to a discrete set of values. You can think of it as a way to reduce the precision of the data. In neural networks, quantization is the process of reducing the precision of the weights and activations. This can be helpful in different ways.\n\nMemory Reduction: In the example of current LLMs, the weights of the feed-forward layers are quite large. Imagine a forward layer weight matrix in Llama 3 70B Model, the weight matrix could be of size 8192 * 8192. In case of float16, this weight matrix would require 8192 * 8192 * 2 = 134,217,728 bytes of memory (approximately 128 MB). This is a lot of memory to store and process, when we consider the fact that the model has multiple such layers. In case we reduced the precision of the weights, we can reduce the load times from memory approximately two, four-fold respectively when using int8 or int4 data types.\nSpeedup: Quantization can also help in speeding up the inference, sometimes even the training process. From computer architecture perspective, the operations on large data types, such as float16 or float32 are expensive and slow. These operations are way faster and cheaper when performed on smaller data types like int8 or float8. When we take a look at the current state-of-the-art GPU Nvidia H100 and its datasheet, we can see that the performance of the GPU Tensor Cores linearly increases with the decrease in the data type size.\n\nNow, that we have a basic understanding on why quantization is important, let’s take a look at how quantization works.\nImportant To simplify things, we will only be looking at quantization to a lower precision data type, that exists in the PyTorch framework, to avoid hassles of binary operations. That is, from torch.float16 to torch.int8. Also, we will be considering a method called Linear Quantization. This is the most common method of quantization."
  },
  {
    "objectID": "articles/quantization-aware-training.html#how-does-quantization-work",
    "href": "articles/quantization-aware-training.html#how-does-quantization-work",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "If we think of int8 as a data type, it can store values in the range of [-128, 127]. However, our weights and activations in float16 have a range of [-65504, 65504]. Also, this range in float16 is not uniformly distributed, therefore accommodating a lot more possible values. To quantize the weights and activations, we need to map the values in float16 to the range of int8. This can be done by the following steps:\n\nMin and Max Calculation: We need to calculate the minimum and maximum values in the data. This will tell us what values map to int8.min and int8.max.\n\n Figure 1: Visual representation of min and max calculation, W_max and W_min are the maximum and minimum values in tensor to be quantized, these values then map to int8.max and int8.min respectively.\n\nZero Point Calculation: We can think of zero point as the point where the float16 value of 0 lies in the int8 data type. This basically maps the real number r=0 to a quantized integer.\n\n Figure 2: Visual representation of zero point calculation. Z on the quantized axis is the zero point, and represents where the r=0.0 lies on the quantized axis.\n\nScale Calculation: The scale basically tells us, how much each unit in the quantized data type represents in the original data type. Imagine a scale of 1.0, this means that each unit in the quantized data type represents 1.0 in the original data type. The larger the scale, the larger is the original input range.\n\nAfter these steps, we have everything we need to quantize and dequantize the data. With r being the real number, q being the quantized number, Z being the zero point, and S being the scale, the quantization and dequantization can be done by the following equations:\n\\[\nq = \\text{round}\\left(\\frac{r}{S}\\right) + Z\n\\]\n\\[\nr = (q - Z) \\cdot S\n\\]\nWith some additional math, we can also derive the scale and zero point equations from the min and max values.\n\\[\nS = \\frac{W_{max} - W_{min}}{Q_{max} - Q_{min}} = \\frac{W_{max} - W_{min}}{127 - (-128)}\n\\]\n\\[\nZ = \\text{round}\\left(Q_{min} - \\frac{W_{min}}{S}\\right) = \\text{round}\\left(-128 - \\frac{W_{min}}{S}\\right)\n\\]"
  },
  {
    "objectID": "articles/quantization-aware-training.html#implementation",
    "href": "articles/quantization-aware-training.html#implementation",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "import torch\nfrom collections import namedtuple\n\nQTensor = namedtuple(\n    \"QTensor\", [\"tensor\", \"scale\", \"zero_point\"]\n)  # we need to track the scale and zero point to dequantize the tensor later\n\n\ndef quantize_tensor(tensor: torch.Tensor) -&gt; QTensor:\n    W_min = tensor.min()\n    W_max = tensor.max()\n\n    Q_min = torch.iinfo(torch.int8).min  # Get the minimum value of the int8 data type\n    Q_max = torch.iinfo(torch.int8).max  # Get the maximum value of the int8 data type\n\n    S = (W_max - W_min) / (Q_max - Q_min)  # Calculate the scale\n    Z = torch.round(Q_min - (W_min / S))  # Calculate the zero point\n\n    quantized_tensor = torch.round(tensor / S) + Z  # Quantize the tensor\n\n    return QTensor(\n        tensor=quantized_tensor.to(torch.int8), scale=S, zero_point=Z\n    )  # Return the quantized tensor, scale, and zero point\n\n\ndef dequantize_tensor(q_tensor: QTensor) -&gt; torch.Tensor:\n    return (\n        q_tensor.tensor.to(torch.float16) - q_tensor.zero_point\n    ) * q_tensor.scale  # simply compute the real value from the already computed data\n\n\nYou might have noticed that we lose quite a lot of information while quantizing the tensor. This might lead to a precision loss, which can be detrimental to the performance of the model. Imagine a scenario where our input tensor distribution looks like the following:\n Figure 3: A pretty common distribution of weights, where most of the values are centered around 0.0.\nNow imagine, we have a single data-point, which is far from the distribution, let’s say W_max=1000.0. If we try to quantize this tensor, the scale would be very large, therefore a distance of 1 in the quantized data would represent a very large distance in the original unquantized data. But remember, our input tensor is distributed around 0.0, with most values lying in the range of [-10.0, 10.0]. This means that most of these values would be quantized to the same value, therefore losing a lot of information leading to a loss in performance.\nTo fix this issue, we can use a method called Clipping. This method involves clipping the values of the tensor to a certain range, and then quantizing the tensor. Our PyTorch implementation can be extended to include this method by the following:\ndef quantize_tensor(\n    tensor: torch.Tensor, clip_min: float | None = None, clip_max: float | None = None\n) -&gt; QTensor:\n    if clip_min or clip_max: # check if atleast one of the clip values is provided\n        tensor = torch.clamp(tensor, clip_min, clip_max)\n\n    W_min = tensor.min()\n    W_max = tensor.max()\n\n    Q_min = torch.iinfo(torch.int8).min  # Get the minimum value of the int8 data type\n    Q_max = torch.iinfo(torch.int8).max  # Get the maximum value of the int8 data type\n\n    S = (W_max - W_min) / (Q_max - Q_min)  # Calculate the scale\n    Z = torch.round(Q_min - (W_min / S))  # Calculate the zero point\n\n    quantized_tensor = torch.round(tensor / S) + Z  # Quantize the tensor\n\n    return QTensor(\n        tensor=quantized_tensor.to(torch.int8), scale=S, zero_point=Z\n    )  # Return the quantized tensor, scale, and zero point\nThis is the only required change to the implementation. In this implementation, we choose the clipping values manually, but in production cases, the clipping values are usually computed from the data distribution via different methods, such as Percentile Clipping, or even optimization methods such as minimizing the KL Divergence between the original and dequantized distribution. This process is called Calibration.\n\n\n\nAnother method to improve the performance of the quantized model is called Quantization Granularity. With the above implementation, we are computing the scale and zero point for the entire tensor. We could improve on this, by computing these values for a sub-part of the tensor. There are different variants of tensor splitting, such as Per Channel, Per Token, etc. The only difference between these variants is across which dimension is the zero point and scale computed. This can further lead to a better performance of the model, with cost of only a few extra bytes in memory. To save time and space, we will not be implementing these methods from scratch here, but just have this in mind when currently used quantization schemes are shown."
  },
  {
    "objectID": "articles/quantization-aware-training.html#quantization-aware-training-1",
    "href": "articles/quantization-aware-training.html#quantization-aware-training-1",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "With this out of the way, we can finally take a look at the concept of Quantization Aware Training. To further improve the performance of the quantized model at inference time, we can use the concept of Quantization Aware Training. This method involves making the model used to quantized weights, activations respectively. This involves training or fine-tuning the model with something called Fake Quantization. This is a method to simulate the quantization process during training. This is accomplished by doing the following:\n\nOriginal weights are stored in the original data type, such as float16.\nComputation is done in the original data type.\nAfter we load the weights, we quantize them and then dequantize back. This is done to simulate the loading of integer weights and their dequantization done during the inference.\nThe activations of the previous layer can be quantized, then dequantized back to get the values that the model would be using during the inference. This depends whether we’re doing both activation and weight quantization, or only weight quantization.\nWe then backpropagate the gradient w.r.t. the dequantized weights, and update the original weights.\n\nYou might be wondering, why do we compute with the dequantized weights, but update the original weights? If you remember our implementation, to dequantize the tensor, we round the values to the nearest integer. This means that if the gradient w.r.t. the dequantized weights is used to update the dequantized weights, the change could be too small to be further visible in the integer representation, therefore we would lose the update information. We can think of this as passing the gradient to the original weights. This is called STE or Straight Through Estimator.\nIt’s easier to see this in a diagram:\n Figure 4: Visual representation of Quantization Aware Training. In this diagram, we can see that the weights are stored in the original data type, and are quantized and dequantized during the forward pass. This simulates the inference process, where the weights are loaded from memory and then dequantized. During the backward pass, we pass the gradient to the original weights, which are then updated."
  },
  {
    "objectID": "articles/quantization-aware-training.html#implementation-1",
    "href": "articles/quantization-aware-training.html#implementation-1",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "To properly implement this, we would like to replace all of the torch.nn.Linear layers with our own custom implementation. This custom implementation would involve the following:\n\nQuantizing the weights\nDequantizing the weights.\nComputing the forward pass with the dequantized weights.\nBackpropagating the error w.r.t. the dequantized weights.\nUpdating the original weights with the gradient.\n\nTo do this, we can register a custom autograd function in PyTorch.\nclass FakeQuantizeFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx: torch.autograd.function.FunctionCtx,\n        tensor: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        W_min, W_max = tensor.min(), tensor.max()\n        Q_min, Q_max = torch.iinfo(torch.int8).min, torch.iinfo(torch.int8).max\n        scale = (W_max - W_min) / (Q_max - Q_min)\n        zero_point = torch.round(-Q_min - (W_min / scale))\n\n        # Quantize\n        quantized = torch.round(tensor / scale) + zero_point\n        quantized = torch.clamp(quantized, Q_min, Q_max)\n\n        # Dequantize\n        dequantized = (quantized - zero_point) * scale\n\n        # Save mask for backward pass\n        mask = (quantized &gt;= Q_min) & (quantized &lt;= Q_max)\n        ctx.save_for_backward(mask)\n\n        return dequantized\n\n    @staticmethod\n    def backward(\n        ctx: torch.autograd.function.FunctionCtx, \n        grad_output: torch.Tensor\n    ) -&gt; tuple[torch.Tensor]:\n        (mask,) = ctx.saved_tensors\n        return grad_output * mask\nThis function is used to simulate the quantization process during training. Note the mask variable, this is used to store a boolean mask of values that weren’t clipped during the forward pass. Therefore, values that were clipped are not updated in the backward pass. This helps simulate the inference process and stabilizes the training process.\nAfter this, we can create a torch.nn.Module that encapsulates the FakeQuantizeFunction and replace all of the torch.nn.Linear layers with this module. We can do this using a simple utility function.\nclass QuantizedLinear(torch.nn.Linear):\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        # Quantize weights\n        quantized_weight = FakeQuantizeFunction.apply(self.weight)\n\n        # Use quantized weights for the linear operation\n        return torch.nn.functional.linear(input, quantized_weight, self.bias)\ndef replace_layers_with_quantized(model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"Replaces all Linear layers with QuantizedLinear layers\"\"\"\n    for name, module in model.named_children():\n        if isinstance(module, torch.nn.Linear):\n            setattr(model, name, QuantizedLinear(\n                in_features=module.in_features,\n                out_features=module.out_features,\n                bias=module.bias is not None\n            ))\n            # Copy the weights and bias\n            getattr(model, name).weight.data = module.weight.data\n            if module.bias is not None:\n                getattr(model, name).bias.data = module.bias.data\n        else:\n            replace_layers_with_quantized(module)\n    return model\nNow, we can create our own model and use this utility function to do QAT.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(784, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 10)\n)\n\nmodel = replace_layers_with_quantized(model)\n\n# Now we can train the model as usual\n# ..."
  },
  {
    "objectID": "articles/quantization-aware-training.html#how-to-do-this-in-practice",
    "href": "articles/quantization-aware-training.html#how-to-do-this-in-practice",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "As you might have noticed, we have written quite a lot of code, which doesn’t handle a lot of edge cases and is rather simple. We can use PyTorch’s AO library to do this for us. This library provides a QAT module, which provides functionality for different quantization schemes.\nThe examples shown were purely educational. In production, different quantization schemes are used to improve the performance of the model. AO currently provides 2 different quantization schemes (note the different granularities of quantization, as discussed in the Quantization Granularity section), which are:\n\nint8 per token dynamic activation quantization with int4 per group weight quantization: This method quantizes weights to int8 and activations to int4. Then, computation is done in original data type, that is float16 usually. This is a good starting point for quantization aware training.\nint4 per group weight quantization: This method quantizes weights to int4, but keeps the activations in float16. Then, weights are dequantized on the fly during the matmul kernel call. This is just to optimize the latency and performance of the model.\n\nTo reproduce our example with AO, you can use the following code:\nimport torch\nfrom torchao.quantization.qat import Int8DynActInt4WeightQATQuantizer\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(784, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 10)\n).cuda()\n\n# Quantizer for int8 dynamic per token activations +\n# int4 grouped per channel weights, only for linear layers\nqat_quantizer = Int8DynActInt4WeightQATQuantizer()\n\n# Insert \"fake quantize\" operations into linear layers.\nmodel = qat_quantizer.prepare(model)\n\n# Now we can train the model as usual\n# ...\nRecalling our example, we have replaced the torch.nn.Linear layers with our own custom implementation. Equivalent of this is done by the qat_quantizer.prepare(model) method. This method inserts the FakeQuantizeFunction into the linear layers, and replaces the weights in the matrix multiplication with the quantized weights.\nHowever, in case of inference, we do not want to do these steps. We only need to dequantize the weights and do the computation, as the weights are already quantized when loaded from memory. We haven’t implemented this in our example, as its not relevant to the concept, but is required in production setting. AO provides a qat_quantizer.convert(model) method, which does this for us. We can use the following code to achieve this:\n# Convert the model to the quantized model\n# This replaces all the \"fake quantize\" operations with the actual quantize operations\nmodel = qat_quantizer.convert(model)\n\n# Now we can use the model for inference\n# ..."
  },
  {
    "objectID": "articles/quantization-aware-training.html#tips-and-tricks",
    "href": "articles/quantization-aware-training.html#tips-and-tricks",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "Finetuning: finetuning the model with QAT is usually a better approach then training the model from scratch.\nLayers: quantizing only some layers is a good approach, some layers are influenced more by the quantization process. Try experimenting with replacing only some layers. Replacing the later layers is usually better than replacing the earlier layers. Also, in general it’s not a good approach to quantize critical layers, such as attention. A good approach is to quantize the feed-forward layers, as those are the ones that require the most memory."
  },
  {
    "objectID": "articles/quantization-aware-training.html#conclusion",
    "href": "articles/quantization-aware-training.html#conclusion",
    "title": "Quantization Aware Training",
    "section": "",
    "text": "In this tutorial, we have looked at the concept of quantization aware training, how does it work in depth, its benefits and how to implement it from scratch in PyTorch, and how to use AO to do this for us, which is a lot more efficient approach."
  },
  {
    "objectID": "articles/on_overdoing.html",
    "href": "articles/on_overdoing.html",
    "title": "On life",
    "section": "",
    "text": "On struggling\nThis blog is a bit different than rest. This is going to be bunch of random nonsense, about topics that have been going through my mind. Mostly about struggling and balancing. Recently I have been more productive as ever, working long hours every day, shipping amazing stuff at work. When I’m not working, I read. Papers, blogs, articles, everything. I consume more knowledge as ever before. But despite that, I have been struggling,comparing myself, to others, to what I could be, to anything and anyone.\nSometimes, it leaves me wondering, is it even worth it? What benefit lies there in learning if there is a plethora of people infinitely smarter and better than me? Call that an impostor syndrome if you will. When do I get to a point “I see and I understand”. However, this struggle didn’t make me stop learning, the other way around.\n\n\n\nOn realizing\nA saying says “Journey is much more beatiful than the destination”, and the same applies in this situation. I was preparing for a lecture on on a course by my friend Zach. The topic is large scale expert-parallelism. I explicitly asked for this topic, as I wasn’t entirely familiar with it, I didn’t have an in-depth grasp. I wanted to learn, to understand. And that’s the thing that made it beautiful, I ran benchmarks, read papers, reimplemented stuff from scratch, all just to understand, to have the intuition. I used to wake up in the middle of the night, realizing a mistake made the day before, noticing a thing overlooked.\nThen I was done, I understood the limitations, derived the equations, saw the benchmarks prove my theorems. It was beautiful and I was satisfied… And then I wasn’t anymore. I realized, trying to find another thing to pursue, to obsess over, that the journey makes the experience thrilling, not the goal, understanding is not the purpose, learning is.\n\n\n\nOn balancing\nAnd so I was stuck in this endless cycle of feeling bad for not understanding or feeling bad for not learning. Trying to balance the scales. There has to be balance to find, and I found it? The lecture, if any, to be taken from this collection of my thoughts, is that pursuing goals is healthy, it’s in our nature and who are we to dispute that. It is in our hands to feel good about the pursue, feel good about not knowing and not understanding. Because from this lack of understanding comes the thrill of pursuing that knowledge. We should pursue knowledge, we should move forward. Though, this pursuit shouldn’t be fueled by comparison of ourselves to others, it HAS to be fueled by our own willpower. The comparison is what makes you struggle, the comparison kills the joy of learning.\nIt’s totally okay to be worse than others, it’s not okay to not try to be better.\nFin.\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]