<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-12-19">
<meta name="description" content="In-depth guide to implementing quantization aware training in PyTorch">

<title>Quantization Aware Training â€“ Matej Sirovatka</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-listing/list.min.js"></script>
<script src="../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c617a82da2177856a622e6615d2e6261.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-posts-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: [{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      
      searchColumns: [],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-posts-listing'] = new List('listing-posts-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Matej Sirovatka</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Quantization Aware Training</h1>
</div>

<div>
  <div class="description">
    In-depth guide to implementing quantization aware training in PyTorch
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 19, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="quantization-aware-training" class="level1">
<h1>Quantization Aware Training</h1>
<p>In this tutorial, we will be looking at the concept of quantization aware training, how does it work in depth, its benefits and how to implement it in PyTorch. To properly understand this concept, proper understanding of quantization basics is required.</p>
<section id="what-is-quantization" class="level2">
<h2 class="anchored" data-anchor-id="what-is-quantization">What is Quantization?</h2>
<p>Formally, quantization is the process of constraining an input from a continuous or otherwise large set of values to a discrete set of values. You can think of it as a way to reduce the precision of the data. In neural networks, quantization is the process of reducing the precision of the weights and activations. This can be helpful in different ways.</p>
<ol type="1">
<li><p><strong>Memory Reduction:</strong> In the example of current LLMs, the weights of the feed-forward layers are quite large. Imagine a forward layer weight matrix in Llama 3 70B Model, the weight matrix could be of size <code>8192 * 8192</code>. In case of <code>float16</code>, this weight matrix would require <code>8192 * 8192 * 2 = 134,217,728</code> bytes of memory (approximately 128 MB). This is a lot of memory to store and process, when we consider the fact that the model has multiple such layers. In case we reduced the precision of the weights, we can reduce the load times from memory approximately two, four-fold respectively when using <code>int8</code> or <code>int4</code> data types.</p></li>
<li><p><strong>Speedup:</strong> Quantization can also help in speeding up the inference, sometimes even the training process. From computer architecture perspective, the operations on large data types, such as <code>float16</code> or <code>float32</code> are expensive and slow. These operations are way faster and cheaper when performed on smaller data types like <code>int8</code> or <code>float8</code>. When we take a look at the current state-of-the-art GPU Nvidia H100 and its <a href="https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet">datasheet</a>, we can see that the performance of the GPU Tensor Cores linearly increases with the decrease in the data type size.</p></li>
</ol>
<p>Now, that we have a basic understanding on why quantization is important, letâ€™s take a look at how quantization works.</p>
<p><strong><em>Important</em></strong> To simplify things, we will only be looking at quantization to a lower precision data type, that exists in the PyTorch framework, to avoid hassles of binary operations. That is, from <code>torch.float16</code> to <code>torch.int8</code>. Also, we will be considering a method called <code>Linear Quantization</code>. This is the most common method of quantization.</p>
</section>
<section id="how-does-quantization-work" class="level2">
<h2 class="anchored" data-anchor-id="how-does-quantization-work">How does Quantization work?</h2>
<p>If we think of <code>int8</code> as a data type, it can store values in the range of <code>[-128, 127]</code>. However, our weights and activations in <code>float16</code> have a range of <code>[-65504, 65504]</code>. Also, this range in <code>float16</code> is not uniformly distributed, therefore accommodating a lot more possible values. To quantize the weights and activations, we need to map the values in <code>float16</code> to the range of <code>int8</code>. This can be done by the following steps:</p>
<ol type="1">
<li><strong>Min and Max Calculation:</strong> We need to calculate the minimum and maximum values in the data. This will tell us what values map to <code>int8.min</code> and <code>int8.max</code>.</li>
</ol>
<p><img src="../quantization-aware-training/media/m.png" class="img-fluid" alt="Min and Max Calculation"> <em>Figure 1: Visual representation of min and max calculation, <code>W_max</code> and <code>W_min</code> are the maximum and minimum values in tensor to be quantized, these values then map to <code>int8.max</code> and <code>int8.min</code> respectively.</em></p>
<ol start="2" type="1">
<li><strong>Zero Point Calculation:</strong> We can think of zero point as the point where the <code>float16</code> value of <code>0</code> lies in the <code>int8</code> data type. This basically maps the real number <code>r=0</code> to a quantized integer.</li>
</ol>
<p><img src="../quantization-aware-training/media/zp.png" class="img-fluid" alt="Zero Point Calculation"> <em>Figure 2: Visual representation of zero point calculation. <code>Z</code> on the quantized axis is the zero point, and represents where the <code>r=0.0</code> lies on the quantized axis.</em></p>
<ol start="3" type="1">
<li><strong>Scale Calculation:</strong> The scale basically tells us, how much each unit in the quantized data type represents in the original data type. Imagine a scale of <code>1.0</code>, this means that each unit in the quantized data type represents <code>1.0</code> in the original data type. The larger the scale, the larger is the original input range.</li>
</ol>
<p>After these steps, we have everything we need to quantize and dequantize the data. With <code>r</code> being the real number, <code>q</code> being the quantized number, <code>Z</code> being the zero point, and <code>S</code> being the scale, the quantization and dequantization can be done by the following equations:</p>
<p><span class="math display">\[
q = \text{round}\left(\frac{r}{S}\right) + Z
\]</span></p>
<p><span class="math display">\[
r = (q - Z) \cdot S
\]</span></p>
<p>With some additional math, we can also derive the scale and zero point equations from the min and max values.</p>
<p><span class="math display">\[
S = \frac{W_{max} - W_{min}}{Q_{max} - Q_{min}} = \frac{W_{max} - W_{min}}{127 - (-128)}
\]</span></p>
<p><span class="math display">\[
Z = \text{round}\left(Q_{min} - \frac{W_{min}}{S}\right) = \text{round}\left(-128 - \frac{W_{min}}{S}\right)
\]</span></p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>QTensor <span class="op">=</span> namedtuple(</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"QTensor"</span>, [<span class="st">"tensor"</span>, <span class="st">"scale"</span>, <span class="st">"zero_point"</span>]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># we need to track the scale and zero point to dequantize the tensor later</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantize_tensor(tensor: torch.Tensor) <span class="op">-&gt;</span> QTensor:</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    W_min <span class="op">=</span> tensor.<span class="bu">min</span>()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    W_max <span class="op">=</span> tensor.<span class="bu">max</span>()</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    Q_min <span class="op">=</span> torch.iinfo(torch.int8).<span class="bu">min</span>  <span class="co"># Get the minimum value of the int8 data type</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    Q_max <span class="op">=</span> torch.iinfo(torch.int8).<span class="bu">max</span>  <span class="co"># Get the maximum value of the int8 data type</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> (W_max <span class="op">-</span> W_min) <span class="op">/</span> (Q_max <span class="op">-</span> Q_min)  <span class="co"># Calculate the scale</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> torch.<span class="bu">round</span>(Q_min <span class="op">-</span> (W_min <span class="op">/</span> S))  <span class="co"># Calculate the zero point</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> torch.<span class="bu">round</span>(tensor <span class="op">/</span> S) <span class="op">+</span> Z  <span class="co"># Quantize the tensor</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> QTensor(</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        tensor<span class="op">=</span>quantized_tensor.to(torch.int8), scale<span class="op">=</span>S, zero_point<span class="op">=</span>Z</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># Return the quantized tensor, scale, and zero point</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dequantize_tensor(q_tensor: QTensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        q_tensor.tensor.to(torch.float16) <span class="op">-</span> q_tensor.zero_point</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">*</span> q_tensor.scale  <span class="co"># simply compute the real value from the already computed data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="clipping" class="level3">
<h3 class="anchored" data-anchor-id="clipping">Clipping</h3>
<p>You might have noticed that we lose quite a lot of information while quantizing the tensor. This might lead to a precision loss, which can be detrimental to the performance of the model. Imagine a scenario where our input tensor distribution looks like the following:</p>
<p><img src="../quantization-aware-training/media/Outliers.png" class="img-fluid" alt="Input Distribution"> <em>Figure 3: A pretty common distribution of weights, where most of the values are centered around 0.0.</em></p>
<p>Now imagine, we have a single data-point, which is far from the distribution, letâ€™s say <code>W_max=1000.0</code>. If we try to quantize this tensor, the scale would be very large, therefore a distance of <code>1</code> in the quantized data would represent a very large distance in the original unquantized data. But remember, our input tensor is distributed around <code>0.0</code>, with most values lying in the range of <code>[-10.0, 10.0]</code>. This means that most of these values would be quantized to the same value, therefore losing a lot of information leading to a loss in performance.</p>
<p>To fix this issue, we can use a method called <code>Clipping</code>. This method involves clipping the values of the tensor to a certain range, and then quantizing the tensor. Our PyTorch implementation can be extended to include this method by the following:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantize_tensor(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    tensor: torch.Tensor, clip_min: <span class="bu">float</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>, clip_max: <span class="bu">float</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> QTensor:</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> clip_min <span class="kw">or</span> clip_max: <span class="co"># check if atleast one of the clip values is provided</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        tensor <span class="op">=</span> torch.clamp(tensor, clip_min, clip_max)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    W_min <span class="op">=</span> tensor.<span class="bu">min</span>()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    W_max <span class="op">=</span> tensor.<span class="bu">max</span>()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    Q_min <span class="op">=</span> torch.iinfo(torch.int8).<span class="bu">min</span>  <span class="co"># Get the minimum value of the int8 data type</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    Q_max <span class="op">=</span> torch.iinfo(torch.int8).<span class="bu">max</span>  <span class="co"># Get the maximum value of the int8 data type</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> (W_max <span class="op">-</span> W_min) <span class="op">/</span> (Q_max <span class="op">-</span> Q_min)  <span class="co"># Calculate the scale</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> torch.<span class="bu">round</span>(Q_min <span class="op">-</span> (W_min <span class="op">/</span> S))  <span class="co"># Calculate the zero point</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> torch.<span class="bu">round</span>(tensor <span class="op">/</span> S) <span class="op">+</span> Z  <span class="co"># Quantize the tensor</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> QTensor(</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        tensor<span class="op">=</span>quantized_tensor.to(torch.int8), scale<span class="op">=</span>S, zero_point<span class="op">=</span>Z</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># Return the quantized tensor, scale, and zero point</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is the only required change to the implementation. In this implementation, we choose the clipping values manually, but in production cases, the clipping values are usually computed from the data distribution via different methods, such as <code>Percentile Clipping</code>, or even optimization methods such as minimizing the <code>KL Divergence</code> between the original and dequantized distribution. This process is called <code>Calibration</code>.</p>
</section>
<section id="quantization-granularity" class="level3">
<h3 class="anchored" data-anchor-id="quantization-granularity">Quantization Granularity</h3>
<p>Another method to improve the performance of the quantized model is called <code>Quantization Granularity</code>. With the above implementation, we are computing the scale and zero point for the entire tensor. We could improve on this, by computing these values for a sub-part of the tensor. There are different variants of tensor splitting, such as <code>Per Channel</code>, <code>Per Token</code>, etc. The only difference between these variants is across which dimension is the zero point and scale computed. This can further lead to a better performance of the model, with cost of only a few extra bytes in memory. To save time and space, we will not be implementing these methods from scratch here, but just have this in mind when currently used quantization schemes are shown.</p>
</section>
</section>
<section id="quantization-aware-training-1" class="level2">
<h2 class="anchored" data-anchor-id="quantization-aware-training-1">Quantization Aware Training</h2>
<p>With this out of the way, we can finally take a look at the concept of <code>Quantization Aware Training</code>. To further improve the performance of the quantized model at inference time, we can use the concept of Quantization Aware Training. This method involves making the model <em>used to</em> quantized weights, activations respectively. This involves training or fine-tuning the model with something called <code>Fake Quantization</code>. This is a method to simulate the quantization process during training. This is accomplished by doing the following:</p>
<ul>
<li>Original weights are stored in the original data type, such as <code>float16</code>.</li>
<li>Computation is done in the original data type.</li>
<li>After we load the weights, we quantize them and then dequantize back. This is done to simulate the loading of integer weights and their dequantization done during the inference.</li>
<li>The activations of the previous layer can be quantized, then dequantized back to get the values that the model would be using during the inference. This depends whether weâ€™re doing both activation and weight quantization, or only weight quantization.</li>
<li>We then backpropagate the gradient w.r.t. the dequantized weights, and update the original weights.</li>
</ul>
<p>You might be wondering, why do we compute with the dequantized weights, but update the original weights? If you remember our implementation, to dequantize the tensor, we round the values to the nearest integer. This means that if the gradient w.r.t. the dequantized weights is used to update the dequantized weights, the change could be too small to be further visible in the integer representation, therefore we would lose the update information. We can think of this as <em>passing</em> the gradient to the original weights. This is called <code>STE</code> or <code>Straight Through Estimator</code>.</p>
<p>Itâ€™s easier to see this in a diagram:</p>
<p><img src="../quantization-aware-training/media/QAT.png" class="img-fluid" alt="Quantization Aware Training"> <em>Figure 4: Visual representation of Quantization Aware Training. In this diagram, we can see that the weights are stored in the original data type, and are quantized and dequantized during the forward pass. This simulates the inference process, where the weights are loaded from memory and then dequantized. During the backward pass, we pass the gradient to the original weights, which are then updated.</em></p>
</section>
<section id="implementation-1" class="level2">
<h2 class="anchored" data-anchor-id="implementation-1">Implementation</h2>
<p>To properly implement this, we would like to replace all of the <code>torch.nn.Linear</code> layers with our own custom implementation. This custom implementation would involve the following:</p>
<ul>
<li>Quantizing the weights</li>
<li>Dequantizing the weights.</li>
<li>Computing the forward pass with the dequantized weights.</li>
<li>Backpropagating the error w.r.t. the dequantized weights.</li>
<li>Updating the original weights with the gradient.</li>
</ul>
<p>To do this, we can register a custom autograd function in PyTorch.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FakeQuantizeFunction(torch.autograd.Function):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        ctx: torch.autograd.function.FunctionCtx,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        tensor: torch.Tensor,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        W_min, W_max <span class="op">=</span> tensor.<span class="bu">min</span>(), tensor.<span class="bu">max</span>()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        Q_min, Q_max <span class="op">=</span> torch.iinfo(torch.int8).<span class="bu">min</span>, torch.iinfo(torch.int8).<span class="bu">max</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> (W_max <span class="op">-</span> W_min) <span class="op">/</span> (Q_max <span class="op">-</span> Q_min)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        zero_point <span class="op">=</span> torch.<span class="bu">round</span>(<span class="op">-</span>Q_min <span class="op">-</span> (W_min <span class="op">/</span> scale))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Quantize</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        quantized <span class="op">=</span> torch.<span class="bu">round</span>(tensor <span class="op">/</span> scale) <span class="op">+</span> zero_point</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        quantized <span class="op">=</span> torch.clamp(quantized, Q_min, Q_max)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dequantize</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        dequantized <span class="op">=</span> (quantized <span class="op">-</span> zero_point) <span class="op">*</span> scale</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save mask for backward pass</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> (quantized <span class="op">&gt;=</span> Q_min) <span class="op">&amp;</span> (quantized <span class="op">&lt;=</span> Q_max)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        ctx.save_for_backward(mask)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> dequantized</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        ctx: torch.autograd.function.FunctionCtx, </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        grad_output: torch.Tensor</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="bu">tuple</span>[torch.Tensor]:</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        (mask,) <span class="op">=</span> ctx.saved_tensors</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> grad_output <span class="op">*</span> mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function is used to simulate the quantization process during training. Note the <code>mask</code> variable, this is used to store a boolean mask of values that werenâ€™t clipped during the forward pass. Therefore, values that were clipped are not updated in the backward pass. This helps simulate the inference process and stabilizes the training process.</p>
<p>After this, we can create a <code>torch.nn.Module</code> that encapsulates the <code>FakeQuantizeFunction</code> and replace all of the <code>torch.nn.Linear</code> layers with this module. We can do this using a simple utility function.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantizedLinear(torch.nn.Linear):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Quantize weights</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        quantized_weight <span class="op">=</span> FakeQuantizeFunction.<span class="bu">apply</span>(<span class="va">self</span>.weight)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use quantized weights for the linear operation</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.nn.functional.linear(<span class="bu">input</span>, quantized_weight, <span class="va">self</span>.bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> replace_layers_with_quantized(model: torch.nn.Module) <span class="op">-&gt;</span> torch.nn.Module:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Replaces all Linear layers with QuantizedLinear layers"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, module <span class="kw">in</span> model.named_children():</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, torch.nn.Linear):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>            <span class="bu">setattr</span>(model, name, QuantizedLinear(</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>                in_features<span class="op">=</span>module.in_features,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                out_features<span class="op">=</span>module.out_features,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>                bias<span class="op">=</span>module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            ))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Copy the weights and bias</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            <span class="bu">getattr</span>(model, name).weight.data <span class="op">=</span> module.weight.data</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>                <span class="bu">getattr</span>(model, name).bias.data <span class="op">=</span> module.bias.data</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            replace_layers_with_quantized(module)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, we can create our own model and use this utility function to do <code>QAT</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    torch.nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>),</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    torch.nn.ReLU(),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    torch.nn.Linear(<span class="dv">256</span>, <span class="dv">10</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> replace_layers_with_quantized(model)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we can train the model as usual</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="how-to-do-this-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="how-to-do-this-in-practice">How to do this in practice?</h2>
<p>As you might have noticed, we have written quite a lot of code, which doesnâ€™t handle a lot of edge cases and is rather simple. We can use PyTorchâ€™s <a href="https://github.com/pytorch/ao">AO</a> library to do this for us. This library provides a <a href="https://github.com/pytorch/ao/tree/main/torchao/quantization/qat">QAT</a> module, which provides functionality for different quantization schemes.</p>
<p>The examples shown were purely educational. In production, different quantization schemes are used to improve the performance of the model. <code>AO</code> currently provides 2 different quantization schemes (note the different granularities of quantization, as discussed in the <a href="#quantization-granularity">Quantization Granularity</a> section), which are:</p>
<ol type="1">
<li><p><strong>int8 per token dynamic activation quantization with int4 per group weight quantization:</strong> This method quantizes weights to <code>int8</code> and activations to <code>int4</code>. Then, computation is done in original data type, that is <code>float16</code> usually. This is a good starting point for quantization aware training.</p></li>
<li><p><strong>int4 per group weight quantization:</strong> This method quantizes weights to <code>int4</code>, but keeps the activations in <code>float16</code>. Then, weights are dequantized <code>on the fly</code> during the <code>matmul</code> kernel call. This is just to optimize the latency and performance of the model.</p></li>
</ol>
<p>To reproduce our example with <code>AO</code>, you can use the following code:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchao.quantization.qat <span class="im">import</span> Int8DynActInt4WeightQATQuantizer</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    torch.nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    torch.nn.ReLU(),</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    torch.nn.Linear(<span class="dv">256</span>, <span class="dv">10</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>).cuda()</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Quantizer for int8 dynamic per token activations +</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># int4 grouped per channel weights, only for linear layers</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>qat_quantizer <span class="op">=</span> Int8DynActInt4WeightQATQuantizer()</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Insert "fake quantize" operations into linear layers.</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> qat_quantizer.prepare(model)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we can train the model as usual</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Recalling our example, we have replaced the <code>torch.nn.Linear</code> layers with our own custom implementation. Equivalent of this is done by the <code>qat_quantizer.prepare(model)</code> method. This method inserts the <code>FakeQuantizeFunction</code> into the linear layers, and replaces the weights in the matrix multiplication with the quantized weights.</p>
<p>However, in case of inference, we do not want to do these steps. We only need to dequantize the weights and do the computation, as the weights are already quantized when loaded from memory. We havenâ€™t implemented this in our example, as its not relevant to the concept, but is required in production setting. <code>AO</code> provides a <code>qat_quantizer.convert(model)</code> method, which does this for us. We can use the following code to achieve this:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the model to the quantized model</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># This replaces all the "fake quantize" operations with the actual quantize operations</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> qat_quantizer.convert(model)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we can use the model for inference</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="tips-and-tricks" class="level2">
<h2 class="anchored" data-anchor-id="tips-and-tricks">Tips and Tricks</h2>
<ul>
<li><strong>Finetuning:</strong> finetuning the model with <code>QAT</code> is usually a better approach then training the model from scratch.</li>
<li><strong>Layers:</strong> quantizing only some layers is a good approach, some layers are influenced more by the quantization process. Try experimenting with replacing only some layers. Replacing the later layers is usually better than replacing the earlier layers. Also, in general itâ€™s not a good approach to quantize critical layers, such as attention. A good approach is to quantize the feed-forward layers, as those are the ones that require the most memory.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this tutorial, we have looked at the concept of quantization aware training, how does it work in depth, its benefits and how to implement it from scratch in PyTorch, and how to use <code>AO</code> to do this for us, which is a lot more efficient approach.</p>



</section>
</section>

<div class="quarto-listing quarto-listing-container-default" id="listing-posts-listing">
<div class="list quarto-listing-default">

</div>
<div class="listing-no-matching d-none">
No matching items
</div>
</div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/S1ro1">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/matej-sirovatka-45959b256/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>