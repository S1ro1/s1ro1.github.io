---
layout: default
title: Tensor Parallelism in Transformers
description: A guide to tensor parallelism in transformers
date: 2025-04-25
---

## 1. Introduction

With the advent of large language (and now multi-modal) models, there is a growing need for efficient ways to train and serve these models. Models with 100s of billions of parameters are not uncommon and those do not fit into a single GPU.
Tensor parallelism is one of the techniques that can be used to help with this problem. This article will cover the basics of tensor parallelism, how it works and most importantly, how you can use predefined tensor parallelism APIs in ðŸ¤— Transformers.

## 2a. How does tensor parallelism work?

At its core, tensor parallelism involves partitioning the model's weights across multiple devices. Let's consider a simple matrix multiplication, $Y = XW$, which is a fundamental operation in neural networks.
We will be considering common transformer architectures, so the weight matrix **$W$** is of shape **$d\_model \times d\_model$** (For simplicity, we will assume square weight matrices, but this is not a requirement for tensor parallelism and doesn't change anything)
and the input matrix **$X$** is of shape **$batch\_size \times d\_model \times seq\_len$**. Again, we can ignore the batch size for now, as that will only cause duplication of the following operations across its dimension.

We can express the shapes as follows:
$$
W \in \mathbb{R}^{d\_model \times d\_model} 
$$

$$
X \in \mathbb{R}^{batch\_size \times d\_model \times seq\_len}
$$

$$
Y \in \mathbb{R}^{batch\_size \times d\_model \times seq\_len}
$$

### Column-wise Partitioning
1) The weight matrix $W$ is split column-wise across two GPUs, where $W_1$ resides on GPU 1 and $W_2$ on GPU 2. This doesn't require any communication, as it is done at initialization.
$$
W = \begin{bmatrix} W_1 & W_2 \end{bmatrix}
$$

$$
W_1, W_2 \in \mathbb{R}^{d\_model \times \frac{d\_model}{2}}
$$

2) The input matrix $X$ is broadcast to both GPUs (each GPU gets a full copy of $X$). (`Broadcast` operation)

$$
Y = XW = X \begin{bmatrix} W_1 & W_2 \end{bmatrix} = \begin{bmatrix} XW_1 & XW_2 \end{bmatrix} = \begin{bmatrix} Y_1 & Y_2 \end{bmatrix}
$$

3) Each GPU computes a part of the output matrix $Y$.

$$
Y_1 = XW_1 \quad \text{and} \quad Y_2 = XW_2
$$

$$
Y_1, Y_2 \in \mathbb{R}^{batch\_size \times d\_model \times \frac{seq\_len}{2}}
$$

4) The final result $Y$ is obtained by concatenating $Y_1$ and $Y_2$ along the column dimension. (`AllGather` operation)

$$
Y = \begin{bmatrix} Y_1 & Y_2 \end{bmatrix}
$$

### Row-wise Partitioning

1) We split the input matrix $X$ column-wise across two GPUs, where $X_1$ resides on GPU 1 and $X_2$ on GPU 2. (`Scatter` operation)

$$
X = \begin{bmatrix} X_1 & X_2 \end{bmatrix}
$$

$$
X_1, X_2 \in \mathbb{R}^{batch\_size \times seq\_len \times \frac{d\_model}{2}}
$$

2) The weight matrix $W$ is split row-wise, where $W_1$ resides on GPU 1 and $W_2$ on GPU 2. Again, this doesn't require any communication, as each GPU only gets its part of the weight matrix at initialization.

$$
W = \begin{bmatrix} W_1 \\ W_2 \end{bmatrix}
$$

$$
W_1, W_2 \in \mathbb{R}^{\frac{d\_model}{2} \times d\_model}
$$

3) Each GPU computes a part of the output matrix $Y$.

$$
Y_1 = X_1W_1 \quad \text{and} \quad Y_2 = X_2W_2
$$

$$
Y_1, Y_2 \in \mathbb{R}^{batch\_size \times seq\_len \times d\_model}
$$

4) The final result $Y$ is obtained by element-wise addition of $Y_1$ and $Y_2$. (`AllReduce` operation)

$$
Y = Y_1 + Y_2
$$

#TODO Visualization

### Bias

Bias is a vector of shape $d\_model$ and is added to the output after the matrix multiplication. You could notice that the previous equations don't consider bias at all.
Handling bias is pretty simple, we just have to do the following:

- **Column-wise partitioning**: For column-wise partitioning, $Y_1$ and $Y_2$ were half the size of the original shape, therefore bias is split across GPUs, each GPU gets a vector $B_i \in \mathbb{R}^{\frac{d\_model}{2}}$
- **Row-wise partitioning**: Recall that $Y_1$ and $Y_2$ were already the original shape, therefore bias is replicated across GPUs, each GPU gets a vector $B_i \in \mathbb{R}^{d\_model}$ (each GPU gets a full copy of the bias)

### Applying tensor parallelism

Column-wise partitioning compliments row-wise partitioning very well. Remember that row-wise partitioning requires the input to split column-wise, which is exactly the output of column-wise partitioning.
This way, concatenation of $Y_1$ and $Y_2$ after the multiplication step of column-wise partitioning is not requireed - saving communication. After this, row-wise can be applied to each of the shards
from the previous step, only requiring weights to be split across GPUs.

#TODO Visualization


## Tensor Parallelism for transformer models

Average transformer architecture consists of multiple layers, each containing a self-attention and a feed-forward network. We would like to apply tensor parallelism to each of these.
As mentioned earlier, we can apply column-wise partitioning first, then row-wise partitioning.

### Feed-Forward Network

Applying tensor parallelism to feed-forward network is straightforward. As discussed [ealier](#applying-tensor-parallelism), column-wise partitioning followed by row-wise partitioning is a natural fit and is really easy to apply to Feed-Forward Network.
Do not mistake $W1$ and $W2$ for $W_1$ and $W_2$ from the previous sections, $W1$ and $W2$ represent two distinct linear layers, not the same weight matrix split across GPUs.


$$
Y = \text{FFN}(X) = \text{ReLU}(XW1)W2
$$


#TODO Visualization

### Self-Attention

Self-attention might seem a bit tricky at first, though it is actually quite simple and natural. We split attention column-wise, making sure each GPU gets a single attention head. Then we split the output projection row-wise.

$$
Y = \text{Dropout}(\text{Self-Attention}(X)W_O)
$$

#TODO Visualization

## 2b. Sequence Parallelism

We will be talking about sequence parallelism as a parallelism technique used together with tensor parallelism, on layers that classical tensor parallelism doesn't apply to. In particular to LayerNorms and Dropouts.
These 2 layers, require full activations to be present for them to be numerically correct.

As we know, both of these operations require the access to the whole hidden dimension to be applied. We can overcome this by splitting the activations across sequence dimension - henceforth `sequence parallelism`.
In practice, we apply tensor parallelism on layers that aren't affected by this problem (FFN + Self-Attention), then apply sequence parallelism on the following layers (LayerNorm + Dropout) and we repeat this across all layers,
alternating between tensor and sequence parallelism.

This introduces some extra subtleties in communication, as we need to ensure correct transition between tensor and sequence parallelism regions.

- **Tensor Parallel -> Sequence Parallel**: Previously, `AllReduce` was used on the output of row-wise partition. Now, the outputs also have to be split across GPUs across the sequence dimension after being reduced. 
This operation is called `ReduceScatter`.
- **Sequence Parallel -> Tensor Parallel**: Each GPU has only part of the activations after the sequence parallel region, meaning it only has a fraction of the tokens (hidden dimension is full). The following tensor parallel region requires full copy of the activations,
so an `AllGather` operation is required, construct the full activations on all GPUs again.


### Some caveats to note

- Tensor parallelism requires a lot of communication between GPUs, that's why it's usually not used inter-node, but only intra-node, taking advantage of high bandwidth interconnects.
- Bias requires extra handling, different for row-wise vs column-wise partitioning.
- To minimize the communication overhead, column-wise partitioning is applied first, then row-wise partitioning, this way we minimize the amount of data that needs to be sent between GPUs making the approach more efficient.
- Commonly used in conjuction with Sequence Parallelism, as they complement each other very well.
- Can also be used on model embeddings, where row-wise partitioning across vocabulary size is applied.


## 3. Distributed PyTorch basics

As with almost everything these days, PyTorch is the library of choice for this feature. In this section, we will cover some basics of distributed PyTorch and some features you should be familiar with, as they make this all possible.
If you know what `torch.distributed.ProcessGroup`, `torch.distributed.DeviceMesh` and `torch.distributed.DTensor` are, you can skip this section and go to the next one.

PyTorch provides a distributed package that allows you to run PyTorch code across multiple processes, even across multiple machines. In this section, we'll consider that each process is running on a single GPU, all processes are running on the same machine.

### Process Group

Encapsulating these processes is a `torch.distributed.ProcessGroup` abstraction. This class ensures communication and synchronization between processes. You should setup the process group, each time you want to run a distributed code.

```python
rank = int(os.environ["RANK"]) # get the rank of the current process, env variable is set by `torchrun`
device = torch.device(f"cuda:{rank}")
torch.cuda.set_device(device) # set the device of the current process
torch.distributed.init_process_group(backend="nccl", device_id=device) # initialize the process group with a communication backend
```

This is a snippet to setup the process group for a single machine with multiple GPUs. You need to run this code, using a `torchrun` command, as such:

```bash
torchrun --nproc_per_node=8 main.py # This will run the script with 8 processes, each on a different GPU
```
`torchrun` is a abbrevation of `torch.distributed.run`, so you can also use it as:
```bash
python3 -m torch.distributed.run --nproc_per_node=8 main.py
```

This process group should also be destroyed after the script is finished, as it's not needed anymore.

```python
if torch.distributed.is_initialized():
    torch.distributed.destroy_process_group()
```

### Device Mesh

PyTorch provides another handy abstraction, `torch.distributed.DeviceMesh`. For tensor parallelism, DeviceMesh isn't as useful, but it's good to be familiar with it.
It's mostly useful when combining multiple parallelization techniques (more on that in another article). It enables creating multiple sub-meshes on a single ProcessGroup, 
where each sub-mesh is responsible for a different parallelization technique. In Transformers, we do the initalization for you, so you don't need to worry about it.
For tensor parallelism, it is created like this:

```python
device_mesh = torch.distributed.device_mesh.init_device_mesh("cuda", (8, ), mesh_dim_names=("tp, ))
```
Then we can use this device mesh to split the model across GPUs and PyTorch will know how to do the communication.


### DTensor

Last but least, we have `torch.distributed.DTensor`. Abbrevation for Distributed Tensor, it's a tensor subclass that on-top of the usual tensor operations, also handles the distributed logic.
You won't be interacting with it directly, but again, it's a class that whole process of tensor parallelism is built on top of.
Most important part of DTensor that is crucial to understand is the `placement` attribute. It's an attribute that tells PyTorch how is the tensor placed on the devices.
It's an enum with the following possible values:

1) `Shard(dimension)` - Annotates that this `DTensor` is sharded across a given dimension, over the device mesh it was constructed under. For example, if we would like to shard weights for column-wise partitioning, we would do:
```python
weight = ...
weight = DTensor.from_local(weight, device_mesh["tp"], placements=[Shard(0)]) # Shard across the 1st (column-wise) dimension
bias = ...
bias = DTensor.from_local(bias, device_mesh["tp"], placements=[Shard(-1)]) # Shard across the ONLY dimension
```

To give another example, for row-wise partitioning, we would do:
```python
weight = ...
weight = DTensor.from_local(weight, device_mesh["tp"], placements=[Shard(1)]) # Shard across the 2nd (row-wise) dimension
bias = ...
bias = DTensor.from_local(bias, device_mesh["tp"], placements=[Replicate()]) # Replicate bias across all GPUs
```

2) `Replicate()` - Annotates that this `DTensor` is replicated across all devices. Pretty straight-forward, it only creates a full copy of the tensor on each device.
3) `Partial()` - This placement is mostly of no interest to us, it's used to annotate that this tensor is pending a reduction operation.

You can find definitions for all of the different partitioning schemes in this [file](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py).
But wait, why do we use different sharding dimensions for column-wise and row-wise? It's because it's applied on `nn.Linear` weights, where the operation is $Y = XW^T + b$.
Math, as specified before, applies the same.

## 4. Using ðŸ¤— Transformers

All of the methods mentioned above seem to be a lot of work to implement manually. Thankfully, Torch and ðŸ¤— Transformers have got you covered! 
With PyTorch's `torch.distributed` and `DTensor` it's very simple to implement tensor parallelism, though it still requires manual configuration for each model.
This is where ðŸ¤— Transformers comes in. It provides with tensor parallelism pre-configured for some popular models, also providing a simple way to implement it for any other model.

### ðŸ¤— Transformers Tensor Parallelism



```python
from transformers import AutoModelForCausalLM

# model_id = "meta-llama/Meta-Llama-3-8B-Instruct" # uncomment for smaller number of GPUs
model_id = "meta-llama/Llama-4-Scout-17B-16E-Instruct" # better to visualize

model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan="auto)

print(model._tp_plan)
```

This will load the model with tensor parallelism pre-configured for the best performance. The model has an attribute `_tp_plan` which contains information about the tensor parallelism plan, running the above with

```bash
torchrun --nproc_per_node=8 main.py
```

will give a result such as:
```python
{
    "layer.*.self_attn.q_proj": "colwise",
    "layer.*.self_attn.k_proj": "colwise",
    "layer.*.self_attn.v_proj": "colwise",
    "layer.*.self_attn.o_proj": "rowwise",
    ...
}
```

This tells us that the query projection is partitioned column-wise, the key and value projections are partitioned column-wise and the output projection is partitioned row-wise. The model is directly loaded and configured with this plan.

### TODO
- Supported layer types (`src/transformers/integrations/tensor_parallel.py`) - mostly cover replicated, why local and packed
- Support own models 
- Should we first implement it in a stable API or just showcase the current way to do it (setting `model.config.tp_plan` and `supports_tp`)?
- Supported models
- Training/Inference (Accelerate for training)

